#!/usr/bin/env python3
"""
Training Data Preparation

Prepares chip design training data for fine-tuning from multiple sources:
1. EDA documentation (scraped)
2. Verilog repositories (code + comments)
3. Synthetic Q&A pairs (generated by 70B)
4. User interactions (logged conversations)

Output format: JSONL for Ollama fine-tuning or HuggingFace training.
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import logging
import json
from pathlib import Path
from typing import List, Dict
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TrainingDataPreparator:
    """Prepare training data from various chip design sources"""

    def __init__(self, output_file: str = "./data/training/chip_design_training.jsonl"):
        self.output_file = Path(output_file)
        self.output_file.parent.mkdir(parents=True, exist_ok=True)

        self.training_examples = []

    def process_documentation(self, docs_dir: str = "./data/knowledge_base"):
        """
        Process EDA documentation into training examples.

        Creates Q&A pairs from documentation sections.
        """
        logger.info("Processing EDA documentation...")

        docs_path = Path(docs_dir)
        if not docs_path.exists():
            logger.warning(f"Documentation directory not found: {docs_dir}")
            return

        # Process all markdown and text files
        for file_path in docs_path.rglob('*'):
            if file_path.suffix in ['.md', '.txt', '.rst']:
                self._process_doc_file(file_path)

        logger.info(f"Processed documentation: {len(self.training_examples)} examples")

    def _process_doc_file(self, file_path: Path):
        """Process a single documentation file"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            # Split into sections (by headers)
            sections = re.split(r'\n#{1,3}\s+', content)

            for section in sections:
                if len(section.strip()) < 100:  # Skip tiny sections
                    continue

                # Extract section title and content
                lines = section.strip().split('\n', 1)
                title = lines[0] if lines else ""
                text = lines[1] if len(lines) > 1 else section

                if len(text.strip()) > 50:
                    # Create training example
                    self.training_examples.append({
                        'prompt': f"Explain {title}",
                        'response': text.strip(),
                        'source': 'documentation',
                        'file': str(file_path.name)
                    })

        except Exception as e:
            logger.debug(f"Error processing {file_path}: {e}")

    def process_verilog_code(self, repos_dir: str = "./data/training/verilog_repos"):
        """
        Process Verilog repositories into training examples.

        Extracts:
        - Module definitions with comments
        - Code patterns
        - Design idioms
        """
        logger.info("Processing Verilog code...")

        repos_path = Path(repos_dir)
        if not repos_path.exists():
            logger.warning(f"Verilog repos directory not found: {repos_dir}")
            return

        initial_count = len(self.training_examples)

        # Process all .v and .sv files
        for file_path in repos_path.rglob('*.v'):
            self._process_verilog_file(file_path)
        for file_path in repos_path.rglob('*.sv'):
            self._process_verilog_file(file_path)

        added = len(self.training_examples) - initial_count
        logger.info(f"Processed Verilog code: {added} examples")

    def _process_verilog_file(self, file_path: Path):
        """Process a single Verilog file"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            # Extract modules with their comments
            module_pattern = r'(/\*[\s\S]*?\*/|//.*\n)*\s*module\s+(\w+)\s*[\s\S]*?endmodule'
            modules = re.finditer(module_pattern, content)

            for match in modules:
                full_match = match.group(0)
                comments = match.group(1) or ""
                module_name = match.group(2)

                if len(full_match) > 100 and len(full_match) < 5000:  # Reasonable size
                    # Create training example
                    prompt = f"Show me an example of a {module_name} module in Verilog"
                    if comments:
                        prompt = f"Explain and show the {module_name} module"

                    self.training_examples.append({
                        'prompt': prompt,
                        'response': full_match.strip(),
                        'source': 'verilog_code',
                        'file': str(file_path.name),
                        'module': module_name
                    })

        except Exception as e:
            logger.debug(f"Error processing {file_path}: {e}")

    def generate_synthetic_qa(self, count: int = 1000):
        """
        Generate synthetic Q&A pairs about chip design.

        This uses templates and can be enhanced with LLM generation.
        """
        logger.info(f"Generating {count} synthetic Q&A pairs...")

        templates = [
            # Synthesis
            ("What is {process}?", "templates/synthesis_explanation.txt"),
            ("How do I optimize for {metric} in synthesis?", "templates/synthesis_optimization.txt"),
            ("What synthesis tool should I use for {task}?", "templates/tool_recommendation.txt"),

            # Placement
            ("How does placement work in {tool}?", "templates/placement_explanation.txt"),
            ("What affects placement quality?", "templates/placement_factors.txt"),
            ("How do I reduce wirelength in placement?", "templates/wirelength_optimization.txt"),

            # Timing
            ("What is setup time?", "templates/setup_time.txt"),
            ("How do I fix setup violations?", "templates/fix_setup.txt"),
            ("What is clock skew?", "templates/clock_skew.txt"),

            # Power
            ("How do I reduce dynamic power?", "templates/dynamic_power.txt"),
            ("What is clock gating?", "templates/clock_gating.txt"),
            ("How does process node affect power?", "templates/process_power.txt"),
        ]

        # For now, create simple examples
        # In production, you'd use 70B to generate detailed responses
        chip_design_topics = [
            ("synthesis", "Converting RTL to gates"),
            ("placement", "Positioning cells on the chip"),
            ("routing", "Connecting cells with wires"),
            ("timing analysis", "Verifying timing constraints"),
            ("power analysis", "Measuring power consumption"),
            ("floorplanning", "Defining chip area and blocks"),
            ("clock tree synthesis", "Building clock distribution"),
            ("DRC", "Design rule checking"),
            ("LVS", "Layout vs schematic verification"),
        ]

        for topic, description in chip_design_topics:
            self.training_examples.append({
                'prompt': f"What is {topic}?",
                'response': f"{topic.title()} is the process of {description} in chip design...",
                'source': 'synthetic',
                'category': topic
            })

        logger.info(f"Generated synthetic examples")

    def save_training_data(self):
        """Save training data to JSONL format"""
        logger.info(f"Saving training data to {self.output_file}")

        with open(self.output_file, 'w', encoding='utf-8') as f:
            for example in self.training_examples:
                # Convert to Ollama/HuggingFace format
                training_record = {
                    'prompt': example['prompt'],
                    'response': example['response'],
                    'metadata': {
                        'source': example.get('source', 'unknown'),
                        'file': example.get('file', ''),
                    }
                }
                f.write(json.dumps(training_record) + '\n')

        logger.info(f"Saved {len(self.training_examples)} training examples")

        # Save statistics
        stats_file = self.output_file.parent / 'training_data_stats.json'
        stats = self._get_statistics()

        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2)

        logger.info(f"Saved statistics to {stats_file}")

    def _get_statistics(self) -> Dict:
        """Get statistics about training data"""
        sources = {}
        total_prompt_length = 0
        total_response_length = 0

        for example in self.training_examples:
            source = example.get('source', 'unknown')
            sources[source] = sources.get(source, 0) + 1

            total_prompt_length += len(example['prompt'])
            total_response_length += len(example['response'])

        return {
            'total_examples': len(self.training_examples),
            'by_source': sources,
            'avg_prompt_length': total_prompt_length / len(self.training_examples) if self.training_examples else 0,
            'avg_response_length': total_response_length / len(self.training_examples) if self.training_examples else 0,
        }


def main():
    """Prepare all training data"""
    logger.info("="*60)
    logger.info("Chip Design Training Data Preparation")
    logger.info("="*60)

    preparator = TrainingDataPreparator()

    # Process all sources
    preparator.process_documentation()
    preparator.process_verilog_code()
    preparator.generate_synthetic_qa()

    # Save
    preparator.save_training_data()

    # Show statistics
    stats = preparator._get_statistics()
    logger.info("\n" + "="*60)
    logger.info("Training Data Statistics")
    logger.info("="*60)
    logger.info(f"Total examples: {stats['total_examples']}")
    logger.info(f"By source: {json.dumps(stats['by_source'], indent=2)}")
    logger.info(f"Avg prompt length: {stats['avg_prompt_length']:.0f} chars")
    logger.info(f"Avg response length: {stats['avg_response_length']:.0f} chars")

    logger.info("\n✅ Training data preparation complete!")
    logger.info(f"\nOutput file: {preparator.output_file}")
    logger.info("\nNext steps:")
    logger.info("1. Review training data quality")
    logger.info("2. Run: training/finetune_8b_chipdesign.py")
    logger.info("3. This will create llama3:8b-chipdesign model")


if __name__ == "__main__":
    main()
