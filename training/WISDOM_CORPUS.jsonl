{"prompt": "I am analyzing a critical path timing report. The path fails setup timing by 35ps. The data path delay is 465ps. The clock path to the endpoint flip-flop has 110ps latency, while the clock path to the startpoint has 40ps latency. The clock period is 500ps. What is the true severity of this timing problem?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** Setup timing failure of 35ps (WNS = -35ps)\n\n**OBSERVATIONS:**\n1. Data path delay: 465ps\n2. Clock period: 500ps\n3. Startpoint clock latency: 40ps\n4. Endpoint clock latency: 110ps\n5. Clock skew: 110ps - 40ps = 70ps (endpoint clock arrives LATE)\n\n**CRITICAL INSIGHT (Counter-Intuitive):**\n\nPositive clock skew (endpoint late) HELPS setup timing, not hurts it.\n\n**WHY:** Setup timing equation is:\n```\nData_arrival_time < Clock_arrival_time - Setup_margin\n\nData arrives at: Startpoint_clock + Data_delay\n                = 40ps + 465ps = 505ps\n\nClock arrives at endpoint: 110ps + 500ps = 610ps\n\nSetup requirement: 610ps - 10ps (setup time) = 600ps\n\nSlack = 600ps - 505ps = +95ps\n```\n\nBut the tool reports -35ps slack. This means there's additional uncertainty/margin.\n\n**THE TRAP REVEALED:**\n\nWorking backwards from reported -35ps:\n```\nIf slack = -35ps with 70ps positive skew:\nThe skew is HELPING by 70ps\n\nWithout skew (both clocks at 75ps):\nData arrival: 75ps + 465ps = 540ps\nClock arrival: 75ps + 500ps = 575ps\nRequired: 575ps - 10ps = 565ps\nSlack: 565ps - 540ps = +25ps\n\nWait, that's still positive. Let me reconsider with uncertainty:\n\nWith 130ps uncertainty budget:\nRequired arrival: 610ps - 10ps - 130ps = 470ps\nActual arrival: 505ps\nSlack: 470ps - 505ps = -35ps \u2713\n\nIf skew were zero (both at 75ps):\nRequired: 575ps - 10ps - 130ps = 435ps\nActual: 540ps\nSlack: 435ps - 540ps = -105ps (!)\n```\n\n**DIAGNOSIS:**\n\nThe current -35ps violation is WITH 70ps of skew help.\nThe true data path problem is **-105ps** (-35ps - 70ps).\n\nIf you \"fix\" the clock skew (balance the tree), you will EXPOSE a -105ps violation that is currently hidden.\n\n**ACTIONABLE GUIDANCE:**\n\n1. **DO NOT touch the clock tree yet** - the skew is currently helping you\n2. **Fix the data path first:** 105ps improvement needed\n   - Insert 2-3 buffers on critical net (expect 30-40ps gain)\n   - Upsize 2-3 gates from X1 \u2192 X4 (expect 40-50ps gain)\n   - Re-place critical cells closer (expect 20-30ps gain)\n   - Total potential gain: 90-120ps\n3. **After data path is fixed,** THEN balance clock tree\n4. **Verify:** Final slack should be +35ps to +50ps (healthy margin)\n\n**THE PRINCIPLE:**\n\nPositive clock skew (endpoint late) is a TIMING GIFT that masks data path problems. When diagnosing setup violations with significant skew, always ask: \"What would happen if skew were zero?\" The answer reveals the true severity.\n\n**COMMON MISTAKES:**\n1. Seeing large skew and immediately trying to \"fix\" it (exposes hidden violations)\n2. Not recognizing that positive skew helps setup timing\n3. Treating skew as an error when it's actually a temporary solution\n\n**THE WISDOM:** In timing analysis, what you see (\u221235ps) is not always what you get. Positive clock skew is a loan from the clock tree that you'll have to repay later. Fix the data path before balancing clocks, or you'll turn a small problem into a catastrophic one.", "phase": "diagnostic_reasoning", "provenance": "diagnostic_corpus", "provenance_score": 95, "complexity_score": 85, "source": "Diagnostic Reasoning: Clock Skew Masking Effect", "verified": "expert_diagnosis", "category": "diagnostic_wisdom", "total_score": 180}
{"prompt": "A control signal travels from a 2GHz CPU clock domain to a 500MHz peripheral clock domain. The timing tool reports a setup violation of -1500ps. I've tried inserting 10 buffers along the path, but the violation remains exactly -1500ps. What is wrong?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** Catastrophic setup violation of -1500ps that doesn't respond to buffer insertion\n\n**OBSERVATIONS:**\n1. Source clock: 2GHz (period = 500ps)\n2. Destination clock: 500MHz (period = 2000ps)\n3. Clock frequency ratio: 2GHz / 500MHz = 4\u00d7\n4. Violation magnitude: -1500ps = **3\u00d7 the source clock period**\n5. Buffer insertion has ZERO effect on violation\n\n**CRITICAL INSIGHT (The Key):**\n\nWhen buffer insertion doesn't help AT ALL, the problem is not delay\u2014it's **constraints**.\n\n**DIAGNOSIS REASONING:**\n\n**Step 1: Recognize the pattern**\n```\nViolation = -1500ps\nDestination period = 2000ps\n\n-1500ps violation means:\nData must arrive by: 2000ps - 1500ps = 500ps\n\nBut wait\u2014500ps is exactly the SOURCE clock period.\n```\n\n**THE TRAP:** The timing tool is checking this path as if it were a **single-cycle** transfer in the destination domain (2000ps available), but the constraint says data must arrive within **500ps** (source domain period).\n\n**Step 2: Identify constraint type**\n\nThis is a **Clock Domain Crossing (CDC)** between unrelated clocks:\n- Source: 2GHz (fast domain)\n- Destination: 500MHz (slow domain, 4\u00d7 slower)\n\nThe tool is applying **synchronous timing analysis** (single-cycle constraint) when this is actually an **asynchronous crossing**.\n\n**Step 3: Understand why buffers don't help**\n\nBuffer insertion reduces path delay by 10-30ps per buffer.\n\nBut -1500ps violation >> any delay optimization can achieve.\n\nThis proves the violation is **fundamental**, not incremental:\n```\nEven with 50 buffers (impossible): 50 \u00d7 30ps = 1500ps gain\nYou'd need INFINITE buffers to fix a constraint error.\n```\n\n**ROOT CAUSE:**\n\n**Missing CDC constraint.** The path needs one of three constraint types:\n\n**Option 1: Asynchronous CDC (no timing relationship)**\n```tcl\n# Tell tool: these clocks are unrelated, don't check timing\nset_false_path -from [get_clocks clk_2ghz] -to [get_clocks clk_500mhz]\n```\n\n**When to use:** Control signals that pass through a synchronizer (2-FF chain) where metastability is acceptable.\n\n**Option 2: Multi-cycle path**\n```tcl\n# Tell tool: data has 4 cycles to cross (4\u00d7 500MHz period = 8ns)\nset_multicycle_path 4 -from [get_clocks clk_2ghz] -to [get_clocks clk_500mhz]\n```\n\n**When to use:** Synchronous CDC where both clocks are phase-locked (e.g., derived from same PLL) and you know data has multiple destination cycles.\n\n**Option 3: Max delay constraint**\n```tcl\n# Tell tool: data must arrive within 2000ps (one 500MHz cycle)\nset_max_delay 2000 -from [get_pins cpu/data_out] -to [get_pins periph/data_in] -datapath_only\n```\n\n**When to use:** When you want to verify path delay without clock relationship (datapath-only ignores clock skew).\n\n**CHOOSING THE RIGHT FIX:**\n\nFor a control signal from CPU to peripheral:\n\n1. **Is there a synchronizer at the boundary?**\n   - YES \u2192 Use `set_false_path` (async CDC, synchronizer handles metastability)\n   - NO \u2192 Continue to step 2\n\n2. **Are the clocks related (same PLL source)?**\n   - YES \u2192 Use `set_multicycle_path 4` (sync CDC, phase-locked)\n   - NO \u2192 Use `set_false_path` + add synchronizer (async CDC)\n\n**MOST LIKELY:** This is an async CDC that needs `set_false_path`.\n\n**VERIFICATION TEST:**\n\nAfter adding constraint, run:\n```tcl\nreport_timing -from [get_clocks clk_2ghz] -to [get_clocks clk_500mhz]\n```\n\nExpected result:\n```\nNo paths found (correctly excluded by false path constraint)\n```\n\n**ACTIONABLE GUIDANCE:**\n\n1. **Add false path constraint:**\n   ```tcl\n   set_false_path -from [get_clocks clk_2ghz] -to [get_clocks clk_500mhz]\n   ```\n\n2. **Verify synchronizer exists in RTL:**\n   ```verilog\n   // 2-FF synchronizer for async CDC\n   reg sync_ff1, sync_ff2;\n   always @(posedge clk_500mhz) begin\n       sync_ff1 <= cpu_signal;\n       sync_ff2 <= sync_ff1;  // Use sync_ff2 in periph domain\n   end\n   ```\n\n3. **If no synchronizer:** ADD ONE (metastability risk!)\n\n4. **Re-run timing:** -1500ps violation should disappear\n\n**THE PRINCIPLE:**\n\n**When a timing violation doesn't respond to optimization (buffers, upsizing, re-placement), it's not a delay problem\u2014it's a constraints problem.** The magnitude of -1500ps (3\u00d7 source period) is the smoking gun: The tool is checking impossible timing (single-cycle across 4\u00d7 frequency ratio). CDCs need explicit constraints (false path, multicycle, or max delay) because the tool cannot infer the architectural intent.\n\n**COMMON MISTAKES:**\n\n1. **Trying to fix constraint errors with physical optimization** (wasting days on impossible task)\n2. **Not recognizing CDC paths** (any cross-clock path is suspect)\n3. **Adding `set_multicycle_path` to async crossings** (wrong! Multicycle assumes phase relationship)\n4. **Missing synchronizers** (functional correctness issue, not just timing)\n\n**THE WISDOM:** A -1500ps violation that doesn't budge is the timing tool screaming: \"You haven't told me the architectural rules!\" Fix the constraints file, not the netlist. And always\u2014ALWAYS\u2014verify synchronizers exist for async CDC. Timing closure is meaningless if the design is functionally broken.", "phase": "diagnostic_reasoning", "provenance": "diagnostic_corpus", "provenance_score": 95, "complexity_score": 90, "source": "Diagnostic Reasoning: CDC Constraint Error", "verified": "expert_diagnosis", "category": "diagnostic_wisdom", "total_score": 185}
{"prompt": "A critical path has 500ps total delay. The breakdown shows 430ps net delay on a single net called 'decode_enable', and 70ps cell delay across 4 logic gates. How do I diagnose whether this is a fanout problem or a distance problem?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** 86% wire delay (430ps/500ps) on single net\n\n**OBSERVATION:** This is extreme wire delay dominance (>80% = red flag)\n\n**DIAGNOSTIC FORK:** Wire delay has TWO root causes with different fixes:\n\n---\n\n## **FORK 1: HIGH FANOUT (Capacitive Load Problem)**\n\n**Physics:** Wire delay = R \u00d7 C, where C increases linearly with fanout\n- 10 loads: C = 100fF (typical)\n- 50 loads: C = 500fF (high)\n- 100 loads: C = 1000fF (extreme)\n\n**Symptom pattern:**\n- Net is relatively SHORT (200-500\u00b5m)\n- But has MANY sinks (>30 loads)\n- Wire RC: R_short \u00d7 C_high = HIGH delay\n\n**How to check:**\n```tcl\nreport_net -connections decode_enable\n# Look for: \"Fanout: XX\"\n```\n\n**Threshold:**\n- Fanout < 10: Normal\n- Fanout 10-30: Marginal\n- Fanout 30-50: High (likely problem)\n- Fanout > 50: Critical (definitely problem)\n\n**Fix:** **Buffer insertion / net splitting**\n```\nBEFORE (50 fanout):\n  Driver \u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ... (50 loads)\n             \u2502             \u2502\n           Load1         Load2 ... Load50\n  \n  C_total = 50 \u00d7 10fF = 500fF\n  R_driver = 100\u03a9 (driving 500fF)\n  Delay = R \u00d7 C = 100\u03a9 \u00d7 500fF = 50ps\n  \n  BUT: Wire adds another 380ps (long routes to 50 loads)\n  Total net delay: 430ps\n\nAFTER (buffer tree, 2 levels):\n  Driver \u2500\u2500\u2500 BUF1 \u2500\u2500\u252c\u2500\u2500 BUF2 \u2500\u2500\u252c\u2500\u2500 Load1..25\n                    \u2502          \u2514\u2500\u2500 Load26..50\n                    \u2514\u2500\u2500 (25 loads per buffer)\n  \n  C_per_stage = 25 \u00d7 10fF = 250fF\n  Delay per stage = 100\u03a9 \u00d7 250fF = 25ps\n  Buffer delay = 15ps \u00d7 2 = 30ps\n  Wire delay = 50ps (shorter routes)\n  Total: 25ps + 30ps + 50ps = 105ps (75% improvement!)\n```\n\n**Expected gain:** 50-70% delay reduction\n\n---\n\n## **FORK 2: LONG DISTANCE (Resistive Wire Problem)**\n\n**Physics:** Wire delay = R \u00d7 C, where R increases linearly with length\n- 200\u00b5m: R = 10\u03a9 (short, acceptable)\n- 1000\u00b5m: R = 50\u03a9 (medium)\n- 3000\u00b5m: R = 150\u03a9 (long, problematic)\n\n**Symptom pattern:**\n- Net has LOW fanout (5-15 loads)\n- But spans LONG distance (>1.5mm)\n- Wire RC: R_high \u00d7 C_medium = HIGH delay\n\n**How to check:**\n```tcl\nreport_net -geometry decode_enable\n# Look for: \"Total length: XXXX um\"\n```\n\n**Also check cell locations:**\n```tcl\nget_cells -of [get_pins -of [get_nets decode_enable]]\nreport_placement -cells [get_cells ...]\n# Check physical X/Y coordinates\n```\n\n**Threshold:**\n- Length < 500\u00b5m: Normal\n- Length 500-1500\u00b5m: Marginal\n- Length 1500-3000\u00b5m: High (likely problem)\n- Length > 3000\u00b5m: Critical (definitely problem)\n\n**Fix:** **Re-placement (bring cells closer)**\n\nThis is a PLACEMENT failure. Cells on critical path were scattered.\n\n```\nBEFORE:\n  Cell_A (0, 0) \u2500\u2500\u2500\u2500\u2500\u25003mm\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Cell_B (3000, 0)\n  \n  Wire R = 3mm \u00d7 50\u03a9/mm = 150\u03a9\n  Wire C = 3mm \u00d7 0.2fF/\u00b5m = 600fF\n  Delay = 150\u03a9 \u00d7 600fF = 90ps (wire RC)\n  + additional coupling/crosstalk: +100ps\n  Total: ~190ps wire delay (for 10 fanout)\n\nAFTER (re-place at 0.65 density instead of 0.85):\n  Cell_A (0, 0) \u2500\u2500\u2500500\u00b5m\u2500\u2500\u2500\u2192 Cell_B (500, 0)\n  \n  Wire R = 0.5mm \u00d7 50\u03a9/mm = 25\u03a9\n  Wire C = 0.5mm \u00d7 0.2fF/\u00b5m = 100fF\n  Delay = 25\u03a9 \u00d7 100fF = 2.5ps (wire RC)\n  + coupling: +10ps\n  Total: ~12ps wire delay (94% improvement!)\n```\n\n**Expected gain:** 80-90% delay reduction\n\n**But:** Requires re-placement (2-3 day iteration)\n\n---\n\n## **THE DIAGNOSTIC DECISION TREE:**\n\n```\nStep 1: Check FANOUT\n  \u251c\u2500 Fanout > 30?\n  \u2502    YES \u2192 HIGH FANOUT problem\n  \u2502           Fix: Insert buffers (2-3 levels)\n  \u2502           Effort: 4-6 hours\n  \u2502           Gain: 50-70%\n  \u2502           GO TO: Buffer insertion\n  \u2502    NO \u2192 Continue to Step 2\n  \u2502\n  \u2514\u2500 Step 2: Check DISTANCE\n       \u251c\u2500 Length > 1500\u00b5m?\n       \u2502    YES \u2192 LONG DISTANCE problem\n       \u2502           Fix: Re-place cells closer\n       \u2502           Effort: 2-3 days (full iteration)\n       \u2502           Gain: 80-90%\n       \u2502           GO TO: Density reduction + re-place\n       \u2502    NO \u2192 Continue to Step 3\n       \u2502\n       \u2514\u2500 Step 3: BOTH problems (rare)\n            \u251c\u2500 Fanout > 20 AND Length > 1000\u00b5m\n            \u2502   Fix: Re-place first, THEN insert buffers\n            \u2502   Reason: Buffer locations depend on cell placement\n            \u2514\u2500 Neither?\n                 \u2192 Check for OTHER issues:\n                   - Routing congestion forcing detours\n                   - Net routed on high-R layer (M1 instead of M3)\n                   - Crosstalk/coupling to adjacent nets\n```\n\n---\n\n## **FOR YOUR SPECIFIC CASE: 'decode_enable' with 430ps net delay**\n\n**Step 1: Check fanout**\n```tcl\nreport_net decode_enable\n```\n\n**If fanout = 50:**\n\u2192 HIGH FANOUT problem\n\u2192 Insert 2-level buffer tree\n\u2192 Expected: 430ps \u2192 120ps (70% improvement)\n\u2192 Time: 6 hours\n\n**If fanout = 8:**\n\u2192 Not fanout issue, check distance\n\n**Step 2: Check distance**\n```tcl\nreport_net -geometry decode_enable\n```\n\n**If length = 2.5mm:**\n\u2192 LONG DISTANCE problem\n\u2192 Re-place at lower density (0.85 \u2192 0.65)\n\u2192 Expected: 430ps \u2192 50ps (88% improvement)\n\u2192 Time: 2-3 days\n\n**If length = 600\u00b5m:**\n\u2192 Not distance issue either...\n\u2192 Check layer assignment (might be routed on M1 instead of M3)\n\n---\n\n## **ACTIONABLE GUIDANCE:**\n\n**ALWAYS check fanout FIRST** (5 minute check, 6 hour fix if needed)\n\n1. Run: `report_net -connections decode_enable`\n2. If fanout > 30: Insert buffers (quick win)\n3. If fanout < 20: Check distance (longer fix)\n\n**DO NOT re-place blindly** without checking fanout first (wasting 2-3 days when 6-hour fix exists)\n\n---\n\n## **THE PRINCIPLE:**\n\nWire delay = R \u00d7 C has two knobs:\n- **R (resistance)** \u2192 controlled by LENGTH\n- **C (capacitance)** \u2192 controlled by FANOUT\n\nWhen wire delay dominates (>80%), identify which knob is turned too high:\n- High fanout (>30) = C problem = **buffer insertion**\n- Long distance (>1.5mm) = R problem = **re-placement**\n\n**Check fanout first** (5 min diagnostic) before committing to re-placement (2-3 day iteration).\n\n---\n\n## **COMMON MISTAKES:**\n\n1. **Assuming distance without checking fanout** (miss quick wins)\n2. **Inserting buffers on low-fanout nets** (doesn't help R problem)\n3. **Not checking layer assignment** (M1 has 5\u00d7 higher R than M3)\n4. **Trying to fix with cell upsizing** (driver strength doesn't help wire RC)\n\n---\n\n## **THE WISDOM:**\n\n86% wire delay is a DIAGNOSTIC FORK, not a single problem. The fix depends on ROOT CAUSE:\n- If fanout > 30: Buffer insertion solves it in 6 hours\n- If length > 1.5mm: Re-placement solves it in 2-3 days\n\n**Check fanout first.** It's the 5-minute test that can save you 3 days.", "phase": "diagnostic_reasoning", "provenance": "diagnostic_corpus", "provenance_score": 95, "complexity_score": 85, "source": "Diagnostic Reasoning: Wire Delay Fork (Fanout vs Distance)", "verified": "expert_diagnosis", "category": "diagnostic_wisdom", "total_score": 180}
{"prompt": "My design passes setup timing with +50ps slack on all paths. But I have 200 hold violations, all around -15ps. An engineer proposes adding buffers to the failing paths to increase delay. Will this fix the hold violations without breaking setup timing?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** 200 hold violations (~-15ps each) despite passing setup (+50ps slack)\n\n**OBSERVATION:** Hold and setup timing are COUPLED\u2014fixing one can break the other\n\n**CRITICAL INSIGHT (The Trade-off):**\n\nHold timing and setup timing are **inverses** of each other:\n- **Setup** wants paths to be FAST (min delay)\n- **Hold** wants paths to be SLOW (max delay)\n\nAdding delay helps hold but hurts setup. The question is: By how much?\n\n---\n\n## **SETUP VS HOLD EQUATIONS:**\n\n**Setup timing:**\n```\nData_arrival < Clock_arrival + Period - Setup_time\n\nSlack_setup = (Clock + Period) - (Launch_clock + Data_delay) - Setup\n            = Period - Data_delay - Setup (if clocks balanced)\n            \nTo PASS setup: Data_delay must be SMALL\n```\n\n**Hold timing:**\n```\nData_arrival > Clock_arrival + Hold_time\n\nSlack_hold = (Launch_clock + Data_delay) - Clock_arrival - Hold\n           = Data_delay - Hold (if clocks balanced)\n           \nTo PASS hold: Data_delay must be LARGE\n```\n\n**THE CONFLICT:**\n- Setup wants: Data_delay < Period - Setup\n- Hold wants: Data_delay > Hold\n\nThere's a **valid window:**\n```\nHold < Data_delay < Period - Setup\n\nFor 500ps period, 10ps setup, 5ps hold:\n5ps < Data_delay < 490ps (valid window: 485ps)\n```\n\n---\n\n## **YOUR SITUATION:**\n\n**Setup slack = +50ps:**\n```\nCurrent data delay = Period - Setup - 50ps\n                   = 500ps - 10ps - 50ps\n                   = 440ps\n```\n\n**Hold slack = -15ps:**\n```\nCurrent data delay = Hold + (-15ps)\n                   = 5ps - 15ps\n                   = NEGATIVE? No, recalculate:\n                   \nHold slack = Data_delay - Hold\n-15ps = Data_delay - 5ps\nData_delay = -10ps? Impossible.\n\nWait\u2014this means data is arriving TOO FAST for hold timing.\n\nCorrect hold equation:\nSlack_hold = Data_delay - Hold_requirement\n\nIf slack = -15ps:\nData_delay = Hold_requirement - 15ps\n\nAssuming Hold_requirement = 50ps (typical with skew):\nData_delay = 50ps - 15ps = 35ps (very fast path!)\n```\n\n---\n\n## **DIAGNOSIS:**\n\nYour paths have **35ps data delay** (fast):\n- This is GOOD for setup (490ps - 35ps = 455ps margin)\n- This is BAD for hold (35ps - 50ps = -15ps violation)\n\n**Root cause:** Paths are too fast (minimal logic, short wires).\n\n---\n\n## **PROPOSED FIX: Add buffers to slow down paths**\n\n**Analysis:**\n\nEach buffer adds ~15ps delay.\n\nTo fix -15ps hold violation:\n```\nNeed: +15ps additional delay\n= 1 buffer per path\n\nNew data delay: 35ps + 15ps = 50ps\nNew hold slack: 50ps - 50ps = 0ps (PASS!)\n```\n\n**But what happens to setup?**\n```\nOld setup slack: +50ps (at 35ps data delay)\n\nSetup slack = (Period - Setup) - Data_delay\n            = 490ps - Data_delay\n\nOld: 490ps - 35ps = +455ps? No, this doesn't match +50ps...\n\nLet me recalculate assuming clock skew matters:\n\nSetup slack = (Endpoint_clock + Period) - (Launch_clock + Data_delay) - Setup\n\nIf launch_clock = 0ps, endpoint_clock = 40ps:\nSetup slack = (40ps + 500ps) - (0ps + 35ps) - 10ps\n            = 540ps - 35ps - 10ps\n            = +495ps? Still too high.\n\nActually, let me use the REPORTED numbers:\n- Current setup slack: +50ps\n- Current data delay: 35ps\n- Hold violation: -15ps\n\nThis implies:\nSetup budget: Period - Uncertainties - Setup = 485ps (example)\nCurrent delay: 35ps\nSetup slack: 485ps - 35ps - some_skew = +50ps\n\u2192 some_skew = 400ps? That's unrealistic.\n\nLet me restart with TYPICAL values:\n```\n\n**REALISTIC SCENARIO:**\n\nAssuming:\n- Period = 500ps\n- Setup time = 10ps\n- Hold time = 5ps\n- Clock uncertainty = 50ps\n- Path delay = 200ps (typical)\n\n**Setup check:**\n```\nRequired arrival: Period - Setup - Uncertainty = 500 - 10 - 50 = 440ps\nActual arrival: 200ps\nSetup slack: 440ps - 200ps = +240ps (way too high, doesn't match)\n```\n\nLet me use YOUR ACTUAL NUMBERS and work backwards:\n\nIf setup slack = +50ps and path delay = D:\n```\nSetup slack = Required_arrival - D\n+50ps = Required_arrival - D\nRequired_arrival = D + 50ps\n```\n\nIf hold slack = -15ps:\n```\nHold slack = D - Hold_requirement\n-15ps = D - Hold_requirement\nHold_requirement = D + 15ps\n```\n\n**NOW, if we add 1 buffer (+15ps):**\n\nNew delay: D' = D + 15ps\n\nNew hold slack:\n```\n= D' - Hold_requirement\n= (D + 15ps) - (D + 15ps)\n= 0ps (PASS!)\n```\n\nNew setup slack:\n```\n= Required_arrival - D'\n= (D + 50ps) - (D + 15ps)\n= +35ps (still positive!)\n```\n\n**ANSWER:**\n\n**YES, adding 1 buffer (+15ps) will:**\n1. Fix hold violations: -15ps \u2192 0ps \u2713\n2. Reduce setup slack: +50ps \u2192 +35ps (still passing) \u2713\n\nThe setup margin (+50ps) is large enough to \"donate\" 15ps to hold timing.\n\n---\n\n## **GENERAL RULE:**\n\nWhen adding delay to fix hold violations:\n```\n\u0394_setup_slack = -\u0394_delay\n\u0394_hold_slack = +\u0394_delay\n```\n\nFor each ps of delay added:\n- Hold slack IMPROVES by +1ps\n- Setup slack DEGRADES by -1ps\n\n**Safe to add delay if:**\n```\nSetup_slack > Hold_violation\n\nYour case: +50ps > |-15ps| = 15ps\n\u2192 Safe to add up to 50ps delay without breaking setup\n```\n\n---\n\n## **ACTIONABLE GUIDANCE:**\n\n1. **Add 1 buffer per failing path** (+15ps delay)\n2. **Expected result:**\n   - Hold violations: -15ps \u2192 0ps (fixed)\n   - Setup slack: +50ps \u2192 +35ps (degraded but still passing)\n3. **Verify post-fix:** Check that setup slack doesn't go negative\n\n**Tool command:**\n```tcl\n# Insert hold buffers\ninsert_buffer -hold_violations [get_cells {hold_failing_paths}]\n\n# Re-run timing\ntimeDesign -postPlace -hold\nreport_timing -delay_type min  # Hold check\nreport_timing -delay_type max  # Setup check (verify still passing)\n```\n\n---\n\n## **THE PRINCIPLE:**\n\n**Setup and hold timing are a zero-sum game.** Delay added to fix hold is delay subtracted from setup. The key question is: **Do you have setup margin to spare?**\n\nIf Setup_slack > Hold_violation: YES, add delay\nIf Setup_slack < Hold_violation: NO, you have a FUNDAMENTAL problem\n\n**In your case:** +50ps setup slack >> 15ps hold violation\n\u2192 Adding 15ps delay trades 50ps \u2192 35ps setup (acceptable) to gain -15ps \u2192 0ps hold (critical).\n\n---\n\n## **COMMON MISTAKES:**\n\n1. **Adding too much delay** (over-fixing hold, breaking setup)\n   - Fix: Add EXACTLY enough delay to reach 0ps hold slack, no more\n\n2. **Not checking setup after hold fix** (assuming setup is safe)\n   - Fix: ALWAYS re-run setup timing after hold fixes\n\n3. **Using clock skew to fix hold** (tempting but dangerous)\n   - Positive skew helps hold but hurts setup (same trade-off)\n   - Better: Add explicit buffers (cleaner, more controllable)\n\n4. **Ignoring hold until tapeout** (hold violations cause functional failures)\n   - Fix: Check hold timing in EVERY timing iteration\n\n---\n\n## **THE WISDOM:**\n\n**Hold timing and setup timing share a fixed \"timing budget\"** (the clock period). Giving more time to one (adding delay for hold) takes time from the other (reducing setup margin). \n\n**The diagnostic question:** \"Do I have setup margin to trade for hold fixes?\"\n\nYour +50ps setup slack is a **timing reserve** that can be spent on hold fixes. Spending 15ps leaves you with +35ps reserve (healthy).\n\n**But beware:** If you had +10ps setup slack and -15ps hold violations, you'd have a **timing conflict**\u2014no amount of buffering can fix both. That's when you need re-placement or clock tree adjustment.", "phase": "diagnostic_reasoning", "provenance": "diagnostic_corpus", "provenance_score": 90, "complexity_score": 80, "source": "Diagnostic Reasoning: Setup vs Hold Trade-off", "verified": "expert_diagnosis", "category": "diagnostic_wisdom", "total_score": 170}
{"prompt": "I placed a design at 90% density and met all timing goals with +20ps slack. But routing fails with 800 DRC violations due to congestion overflow. My manager says 'timing is passing, just fix the routing\u2014don't touch placement.' Why is this approach doomed to fail?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** Timing passes (+20ps slack) but routing fails (800 DRC violations)\n\n**MANAGER'S ASSUMPTION:** Timing and routing are independent\u2014fix routing without touching placement\n\n**CRITICAL INSIGHT (Why This Fails):**\n\n**Placement determines routing feasibility.** You cannot \"fix routing\" when placement has created impossible congestion.\n\n---\n\n## **THE FUNDAMENTAL PROBLEM:**\n\n**At 90% density:**\n- Cell area consumes 90% of die\n- Routing channels have 10% white space\n- This gives ~20-30% fewer routing tracks than 70% density\n\n**If placement created congestion hotspots:**\n- Local tiles have >100% routing demand\n- Router CANNOT route these tiles (physically impossible)\n- DRC violations are SYMPTOMS of unroutable placement\n\n---\n\n## **WHY \"JUST FIX ROUTING\" FAILS:**\n\n**Attempt 1: Manual routing guidance**\n```tcl\n# Try forcing specific layers\nset_routing_layer_priority M3 M4 M5\n\n# Try reducing wire width\nset_routing_width 0.08um\n```\n\n**Result:** 800 violations \u2192 650 violations (19% improvement, still failing)\n\n**Why:** Routing guidance can reduce overflow by 10-20%, but if demand/supply > 1.2 (20% overflow), no amount of routing tricks can fix it.\n\n**Attempt 2: Allow routing violations**\n```tcl\n# Relax DRC rules (desperate)\nset_routing_rule -min_spacing 0.06um  # Was 0.08um\n```\n\n**Result:** Routes complete, but:\n- Manufacturability risk (spacing violations)\n- Yield loss (shorts between wires)\n- May not pass DRC signoff (tapeout blocker)\n\n**Attempt 3: Re-route 100 times**\n```tcl\nfor {set i 0} {$i < 100} {incr i} {\n    route_design -incremental\n}\n```\n\n**Result:** 800 violations \u2192 750 violations after 100 iterations (6% improvement)\n\n**Time wasted:** 3 days of routing iterations\n\n**Why:** The router is trying to fit 120 wires into 100 available tracks. No algorithm can solve 120 > 100.\n\n---\n\n## **THE BRUTAL MATH:**\n\n**Congestion overflow = demand / supply:**\n\n800 DRC violations suggests:\n```\nAffected tiles: ~50 tiles (800 violations / 16 per tile)\nDemand in hotspots: 1200 tracks\nSupply in hotspots: 1000 tracks\nOverflow: 1200 / 1000 = 1.20 (20% overflow)\n```\n\n**Router cannot create tracks.** It can only:\n1. Distribute demand better (10-15% improvement max)\n2. Use different layers (5-10% improvement max)\n3. Reduce wire width (5% improvement max)\n\n**Total possible: 20-30% improvement**\n\nBut you need 20% overflow eliminated \u2192 routing tricks get you CLOSE but not FIXED.\n\n---\n\n## **WHY TIMING PASSED (+20ps slack) IS MISLEADING:**\n\n**Timing passed because:**\n1. STA assumes wires route successfully\n2. STA uses Manhattan distance (direct routes)\n3. STA doesn't account for congestion detours\n\n**But when routing actually runs:**\n```\nExpected route (Manhattan): 500\u00b5m\nActual route (with detours): 850\u00b5m (70% longer!)\n\nExpected wire delay: 25ps\nActual wire delay: 42ps (+17ps)\n\nTiming after routing: +20ps - 17ps = +3ps (marginal!)\n```\n\n**The +20ps timing margin was a MIRAGE** based on impossible routing assumptions.\n\n---\n\n## **WHAT HAPPENS IF YOU \"FORCE\" ROUTING TO COMPLETE:**\n\nOption 1: Accept DRC violations and tapeout\n\u2192 Chip may not fabricate correctly (shorts, opens)\n\u2192 Yield loss: 30-50%\n\u2192 Cost: $2M-$5M per respin\n\nOption 2: Relax spacing rules\n\u2192 Pass DRC but increase crosstalk\n\u2192 Timing degradation: +15-25ps\n\u2192 Timing now fails: +20ps \u2192 -5ps (violation)\n\u2192 Back to square one, but with worse routing\n\nOption 3: Keep iterating routing\n\u2192 Waste 2 weeks\n\u2192 Still have 200-300 violations (never converges)\n\u2192 Project schedule slips\n\n---\n\n## **THE ROOT CAUSE:**\n\n**90% density at this design's complexity is TOO HIGH.**\n\nEvidence:\n```\nDensity   | Routing success rate (industry data)\n----------|--------------------------------\n70%       | 99% (safe)\n80%       | 95% (marginal)\n85%       | 85% (risky)\n90%       | 60% (high risk)\n95%       | 20% (usually fails)\n```\n\nAt 90%, you're in the \"high risk\" zone. Routing success depends on:\n1. Uniform cell distribution (hard at 90%)\n2. Low local hotspots (placement must be perfect)\n3. Good white space allocation (minimal at 90%)\n\nYour 800 DRC violations prove: **Placement created unroutable hotspots.**\n\n---\n\n## **THE CORRECT FIX:**\n\n**Reduce density \u2192 Re-place:**\n\n```tcl\n# Reduce target density\nsetPlaceMode -place_global_density 0.80  # Was 0.90\n\n# Re-run placement\nplace_design\n\n# Check congestion\nreport_congestion\n\n# Expected result:\n# Congestion hotspots: 1.20 \u2192 0.90 (under capacity!)\n\n# Route\nroute_design\n\n# Expected result:\n# DRC violations: 800 \u2192 50 (fixable with manual tweaks)\n```\n\n**Cost:**\n- Area increase: 90% \u2192 80% = +12.5% die size\n- At 5mm \u00d7 5mm: 25mm\u00b2 \u2192 28mm\u00b2 (+3mm\u00b2)\n- Die cost: +$0.08/chip (negligible for most products)\n\n**Benefit:**\n- Routing succeeds (800 \u2192 50 violations)\n- Timing still passes (+20ps \u2192 +15ps after real routing)\n- Schedule: Save 2 weeks of failed routing iterations\n\n---\n\n## **WHY MANAGER'S APPROACH FAILS:**\n\n**Manager assumes:** Timing and routing are sequential (place \u2192 route)\n\n**Reality:** Timing and routing are COUPLED:\n- Placement creates routing demand distribution\n- Routing must satisfy that demand with available supply\n- If demand > supply (congestion), routing fails\n- No amount of \"routing fixing\" can violate physics (demand > supply)\n\n**The manager is asking you to:** \"Fit 120 cars into a 100-car parking lot without building more parking spaces.\"\n\n**It's impossible.**\n\n---\n\n## **CONVERSATION WITH MANAGER:**\n\n**YOU:** \"We have 800 DRC violations due to 20% routing overflow in 50 tiles. The router cannot create tracks\u2014it can only distribute wires. We need to re-place at 80% density to create 20% more routing tracks.\"\n\n**MANAGER:** \"But timing is passing! Don't break timing.\"\n\n**YOU:** \"The +20ps timing slack is based on Manhattan distance routing, which assumes wires route directly. With congestion, actual routes are 70% longer (detours around hotspots), which costs +17ps. After routing, we'd have +3ps slack\u2014and that's IF routing converges, which it won't with 20% overflow. Re-placing at 80% density keeps timing above +15ps and makes routing feasible.\"\n\n**MANAGER:** \"How long does re-placement take?\"\n\n**YOU:** \"2 days for re-place + route. Compare to: 2 weeks of failed routing iterations + potential respin. The die cost increase is $0.08/chip\u2014negligible compared to schedule risk.\"\n\n**MANAGER (hopefully):** \"OK, re-place at 80%.\"\n\n---\n\n## **ACTIONABLE GUIDANCE:**\n\n1. **Re-place at 80% density**\n   ```tcl\n   setPlaceMode -place_global_density 0.80\n   place_design\n   ```\n\n2. **Run congestion check BEFORE routing**\n   ```tcl\n   report_congestion -hotspots 20\n   # Verify all hotspots < 1.0 (no overflow)\n   ```\n\n3. **Route**\n   ```tcl\n   route_design\n   ```\n\n4. **Expected:** 800 violations \u2192 <50 (98% improvement)\n\n5. **Check timing post-route**\n   ```tcl\n   timeDesign -postRoute\n   # Expected: +15ps to +18ps (still passing)\n   ```\n\n---\n\n## **THE PRINCIPLE:**\n\n**Routing is not independent of placement.** Congestion violations are PLACEMENT failures, not routing failures. The router is a slave to placement\u2014it can only route what placement makes routable.\n\n**At high density (>85%), routability dominates timing.** You cannot \"fix routing\" after placement without re-placing. The attempt wastes 2 weeks and fails.\n\n**The decision:** Spend 2 days re-placing at lower density, or spend 2 weeks failing to route + risk respin?\n\n---\n\n## **COMMON MISTAKES:**\n\n1. **Treating timing and routing as independent** (they're coupled through placement)\n2. **Trusting timing slack before routing** (STA assumes ideal routes)\n3. **Trying to \"fix\" routing without re-placing** (impossible when overflow > 10%)\n4. **Not checking congestion during placement** (by the time routing fails, it's too late)\n\n---\n\n## **THE WISDOM:**\n\n**DRC violations are the router's way of saying: \"Placement gave me an impossible task.\"**\n\nWhen routing fails with 800 violations despite passing timing, the problem is NOT the router\u2014it's the placement. Reducing density by 10% (90% \u2192 80%) creates 25% more routing capacity, which eliminates overflow and allows routing to succeed.\n\n**Don't ask the router to violate physics.** Give it routable placement.\n\nThe +$0.08/chip die cost is the price of routability. Pay it, or pay $2M for a respin.", "phase": "diagnostic_reasoning", "provenance": "diagnostic_corpus", "provenance_score": 95, "complexity_score": 85, "source": "Diagnostic Reasoning: Routing Failure from Placement Congestion", "verified": "expert_diagnosis", "category": "diagnostic_wisdom", "total_score": 180}
{"prompt": "I have a design with 80ps of clock uncertainty budget. The clock tree has 60ps of measured skew. My manager says this is acceptable because 60ps < 80ps. Why is this reasoning flawed, and how does skew actually interact with the uncertainty budget?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** Manager claims 60ps skew < 80ps uncertainty = acceptable\n\n**OBSERVATIONS:**\n1. Clock uncertainty budget: 80ps\n2. Measured clock skew: 60ps\n3. Manager's logic: \"60 < 80, therefore OK\"\n\n**CRITICAL INSIGHT (Counter-Intuitive):**\n\n**Skew and uncertainty are NOT interchangeable.** They affect timing differently:\n\n- **Uncertainty** is a MARGIN for unknown variations (jitter, OCV, etc.)\n- **Skew** is a MEASURED clock arrival difference between flip-flops\n\n**WHY THIS MATTERS:**\n\nSetup timing equation:\n```\nData_arrival < (Clock_at_endpoint + Period) - Setup - Uncertainty\n\nIf skew = 60ps (endpoint clock arrives 60ps late):\n  Required_arrival = (T_launch + 60ps + Period) - Setup - 80ps\n                   = T_launch + Period + 60ps - Setup - 80ps\n                   = T_launch + Period - Setup - 20ps\n\nEffective available time = Period - Setup - 20ps\n```\n\n**THE TRAP:** The manager subtracted skew from uncertainty (80 - 60 = 20ps remaining). This is **wrong**.\n\n**CORRECT ANALYSIS:**\n\nSkew affects WHICH clock edges are being compared, not the uncertainty margin.\n\n**Positive skew (endpoint late):**\n- HELPS setup timing (+60ps more time for data)\n- HURTS hold timing (-60ps less time for hold)\n\n**Uncertainty affects BOTH setup and hold:**\n- Setup: Required_arrival = Period - Setup - Uncertainty\n- Hold: Required_arrival = Hold + Uncertainty\n\nSo the actual timing budget is:\n```\nSetup slack = Period - Data_delay - Setup - Uncertainty + Skew\n            = Period - Data_delay - Setup - 80ps + 60ps\n            = Period - Data_delay - Setup - 20ps\n\nHold slack = Data_delay - Hold - Uncertainty - Skew\n           = Data_delay - Hold - 80ps - 60ps\n           = Data_delay - Hold - 140ps\n```\n\n**THE PROBLEM:** While setup benefits from +60ps skew, hold is now **penalized by -140ps** (80ps uncertainty + 60ps skew).\n\n**DIAGNOSIS:**\n\nThe 60ps skew is **not \"free\"** - it trades setup margin for hold risk. The manager's reasoning is flawed because:\n\n1. Skew doesn't reduce uncertainty (they're independent)\n2. Positive skew helps setup but hurts hold\n3. The real question: \"Can we afford to give up 60ps of hold margin?\"\n\n**ACTIONABLE GUIDANCE:**\n\n1. **Check hold timing:** Are there hold violations?\n   ```tcl\n   report_timing -delay_type min -slack_lesser_than 0\n   ```\n\n2. **If hold is clean (>50ps slack):** The 60ps skew is acceptable\n3. **If hold is marginal (<20ps slack):** The skew is dangerous - balance the clock tree\n\n**THE PRINCIPLE:**\n\n**Clock skew is a zero-sum game:** What setup gains, hold loses. Uncertainty is a safety margin that applies to BOTH. Never confuse the two.\n\n**COMMON MISTAKES:**\n1. Treating skew as \"consuming\" uncertainty budget\n2. Ignoring hold impact when evaluating positive skew\n3. Balancing clock tree without checking if skew was helping setup\n\n**THE WISDOM:** Skew is a loan from hold timing to setup timing. If you have hold margin to spare, positive skew is a gift. If hold is already tight, skew is a disaster waiting to happen.", "phase": "diagnostic_reasoning", "provenance": "wisdom_upgrade", "source": "Wisdom Upgrade: Clock Skew vs Uncertainty Budget Management", "category": "diagnostic_wisdom", "complexity_score": 90, "verified": "expert_diagnosis"}
{"prompt": "A placed design passes setup timing but has 50 hold violations. Your manager says 'just add buffers to the failing paths.' Why is this advice dangerously incomplete, and what is the correct diagnostic procedure?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** 50 hold violations with passing setup timing\n\n**MANAGER'S ADVICE:** \"Add buffers to failing paths\"\n\n**CRITICAL INSIGHT (The Danger):**\n\n**Adding delay to fix hold CAN break setup.** Hold and setup timing are coupled - they share the same data path delay.\n\n**WHY:**\n```\nSetup: Data_delay < Period - Setup_time - Uncertainty\nHold:  Data_delay > Hold_time + Uncertainty\n\nIf you add buffers \u2192 Data_delay increases:\n  - Hold slack IMPROVES (+\u0394 delay)\n  - Setup slack DEGRADES (-\u0394 delay)\n```\n\n**THE DIAGNOSTIC PROCEDURE:**\n\n**Step 1: Verify Setup Margin**\n```tcl\nreport_timing -delay_type max -slack_greater_than 0 | grep \"slack\"\n```\n\nFind the MINIMUM setup slack:\n```\nBest case: +200ps setup slack\nWorst case: +5ps setup slack (DANGER!)\n```\n\n**Step 2: Measure Hold Violations**\n```tcl\nreport_timing -delay_type min -slack_lesser_than 0\n```\n\nFind the MAXIMUM hold violation:\n```\nTypical: -15ps hold violation\nWorst: -50ps hold violation\n```\n\n**Step 3: Calculate Safe Buffer Delay**\n\n```\nSafe_delay_to_add = MIN(Setup_slack, |Hold_violation|)\n\nExample:\n  Setup_slack = +30ps\n  Hold_violation = -20ps\n\n  Safe_delay = MIN(30ps, 20ps) = 20ps\n\n  Add 20ps delay (e.g., 1-2 buffers) to hold-failing paths:\n    - Hold: -20ps \u2192 0ps (FIXED)\n    - Setup: +30ps \u2192 +10ps (still passing)\n```\n\n**Step 4: Check for Hold-Only Paths**\n\nSome paths may have hold violations but DON'T affect setup (e.g., same-cycle paths with minimal logic). These are safe to buffer aggressively.\n\n```tcl\n# Check if path is launch-to-launch (same flip-flop)\nreport_timing -from [get_pins FF/Q] -to [get_pins FF/D] -delay min\n```\n\nIf launch == capture flip-flop: **This is a HOLD-ONLY path** - add buffers freely.\n\n**ACTIONABLE GUIDANCE:**\n\n**Case 1: Large Setup Margin (+100ps)**\n\u2192 Add buffers to all 50 hold violations\n\u2192 Unlikely to impact setup\n\n**Case 2: Small Setup Margin (+20ps)**\n\u2192 **DANGER!** Adding 50ps of buffers will break setup\n\u2192 Option A: Balance clock tree (shift skew to help hold)\n\u2192 Option B: Selectively buffer only worst hold violations\n\u2192 Option C: Increase clock period (if possible)\n\n**Case 3: Hold violations on critical paths**\n\u2192 **CONFLICT!** Path needs to be both fast (setup) and slow (hold)\n\u2192 This is architecturally broken - need to:\n  - Move logic to different clock cycle\n  - Add pipeline stage\n  - Use clock domain crossing with async FIFO\n\n**THE PRINCIPLE:**\n\n**Hold and setup share a fixed timing budget.** Adding delay to fix hold subtracts from setup margin. The diagnostic question: \"How much setup margin can I afford to sacrifice?\"\n\n**COMMON MISTAKES:**\n1. Blindly adding buffers without checking setup impact\n2. Not identifying hold-only paths that are safe to buffer\n3. Not considering clock tree adjustment as alternative\n\n**THE WISDOM:** When fixing hold violations, you're playing a zero-sum game. Every ps of delay you add helps hold but hurts setup. Know your setup margin before you start buffering, or you'll fix hold by breaking setup.", "phase": "diagnostic_reasoning", "provenance": "wisdom_upgrade", "source": "Wisdom Upgrade: Setup/Hold Timing Violation Diagnosis", "category": "diagnostic_wisdom", "complexity_score": 85, "verified": "expert_diagnosis"}
{"prompt": "A critical path has 500ps total delay. The breakdown shows 430ps net delay on a single net called 'decode_enable', and 70ps cell delay across 4 logic gates. How do I diagnose whether this is a fanout problem or a distance problem?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** 86% wire delay (430ps/500ps) on single net\n\n**OBSERVATION:** This is extreme wire delay dominance (>80% = red flag)\n\n**DIAGNOSTIC FORK:** Wire delay has TWO root causes with different fixes:\n\n---\n\n## **STEP 1: CHECK FANOUT FIRST (5 minute diagnostic)**\n\n**Why fanout first?** If it's fanout, you can fix it in 6 hours. If it's distance, you need 2-3 days of re-placement.\n\n**Command:**\n```tcl\nreport_net -connections decode_enable\n# Look for: \"Fanout: XX\"\n```\n\n**Interpretation:**\n- Fanout < 10: Normal (not the problem)\n- Fanout 10-30: Marginal\n- Fanout 30-50: High (likely problem)\n- Fanout > 50: Critical (definitely problem)\n\n**Physics:** Wire delay = R \u00d7 C, where C increases with fanout\n```\n10 loads:  C = 100fF  \u2192 delay = R \u00d7 100fF\n50 loads:  C = 500fF  \u2192 delay = R \u00d7 500fF (5\u00d7 worse!)\n```\n\n**If fanout = 45:**\n\u2192 **HIGH FANOUT problem**\n\u2192 Fix: Insert 2-level buffer tree\n\u2192 Expected: 430ps \u2192 120ps (70% improvement)\n\u2192 Time: 6 hours\n\n**STOP HERE - fix fanout, don't check distance yet**\n\n---\n\n## **STEP 2: IF FANOUT < 20, CHECK DISTANCE**\n\n**Command:**\n```tcl\nreport_net -geometry decode_enable\n# Look for: \"Total length: XXXX um\"\n```\n\n**Interpretation:**\n- Length < 500\u00b5m: Normal (not the problem)\n- Length 500-1500\u00b5m: Marginal\n- Length 1500-3000\u00b5m: High (likely problem)\n- Length > 3000\u00b5m: Critical (definitely problem)\n\n**Physics:** Wire delay = R \u00d7 C, where R increases with length\n```\n200\u00b5m:  R = 10\u03a9   \u2192 delay = 10\u03a9 \u00d7 C\n3000\u00b5m: R = 150\u03a9  \u2192 delay = 150\u03a9 \u00d7 C (15\u00d7 worse!)\n```\n\n**If length = 2.5mm:**\n\u2192 **LONG DISTANCE problem**\n\u2192 Fix: Re-place at lower density (0.85 \u2192 0.65)\n\u2192 Expected: 430ps \u2192 50ps (88% improvement)\n\u2192 Time: 2-3 days\n\n---\n\n## **THE DECISION TREE:**\n\n```\nWire delay = 430ps (86% of path)\n              \u2193\n       Check FANOUT (5 min)\n              \u2193\n        Fanout > 30?\n        \u2199         \u2198\n      YES          NO\n       \u2193            \u2193\n  INSERT       Check DISTANCE\n  BUFFERS          \u2193\n  (6 hours)   Length > 1.5mm?\n                \u2199         \u2198\n              YES          NO\n               \u2193            \u2193\n           RE-PLACE    Check OTHER:\n          (2-3 days)   - Layer (M1 vs M3)\n                       - Routing detours\n                       - Crosstalk\n```\n\n**ACTIONABLE GUIDANCE:**\n\n1. **ALWAYS check fanout FIRST**\n   - 5 minute check\n   - 6 hour fix if needed\n   - Don't waste 3 days when 6-hour fix exists\n\n2. **If fanout < 20, THEN check distance**\n   - Requires re-placement (2-3 day iteration)\n   - Last resort after fanout is ruled out\n\n3. **If BOTH are fine:**\n   - Check layer assignment (M1 has 5\u00d7 higher R than M3)\n   - Check routing congestion (detours add length)\n\n**THE PRINCIPLE:**\n\n**Wire delay = R \u00d7 C has two knobs:**\n- **C (capacitance)** \u2192 controlled by FANOUT \u2192 fix with buffers (hours)\n- **R (resistance)** \u2192 controlled by LENGTH \u2192 fix with re-placement (days)\n\n**Check the quick fix first.** Don't commit to 3 days of re-placement when a 6-hour buffer insertion might solve it.\n\n**THE WISDOM:** 86% wire delay is a diagnostic fork, not a single problem. Fanout and distance both cause high wire delay, but they have radically different fix times. Diagnose fanout first (5 min check, 6 hr fix) before committing to re-placement (2-3 day fix). This one diagnostic decision can save you 3 days.", "phase": "diagnostic_reasoning", "provenance": "wisdom_upgrade", "source": "Wisdom Upgrade: Wire Delay Diagnostic Fork (Fanout vs Distance)", "category": "diagnostic_wisdom", "complexity_score": 90, "verified": "expert_diagnosis"}
{"prompt": "I placed a design at 90% density and met all timing goals with +20ps slack. But routing fails with 800 DRC violations due to congestion overflow. My manager says 'timing is passing, just fix the routing\u2014don't touch placement.' Why is this approach doomed to fail?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** Timing passes (+20ps slack) but routing fails (800 DRC violations)\n\n**MANAGER'S ASSUMPTION:** Timing and routing are independent - fix routing without touching placement\n\n**CRITICAL INSIGHT (Why This Fails):**\n\n**Routing is a slave to placement.** You cannot \"fix routing\" when placement created impossible congestion.\n\n---\n\n## **THE BRUTAL MATH:**\n\n800 DRC violations suggests:\n```\nAffected tiles: ~50 tiles (800 violations / 16 per tile)\nDemand in hotspots: 1200 tracks\nSupply in hotspots: 1000 tracks\nOverflow: 1200 / 1000 = 1.20 (20% overflow)\n```\n\n**The router CANNOT create tracks.** It can only:\n1. Distribute demand better (10-15% improvement max)\n2. Use different layers (5-10% improvement max)\n3. Reduce wire width (5% improvement max)\n\n**Total possible improvement: 20-30%**\n\nBut you need **20% overflow eliminated**. Routing tricks get you CLOSE but not FIXED.\n\n---\n\n## **WHY TIMING PASSED IS MISLEADING:**\n\n**Timing passed because:**\n1. STA assumes wires route successfully\n2. STA uses Manhattan distance (direct routes)\n3. STA doesn't account for congestion detours\n\n**But when routing actually runs:**\n```\nExpected route (Manhattan): 500\u00b5m\nActual route (with detours): 850\u00b5m (70% longer!)\n\nExpected wire delay: 25ps\nActual wire delay: 42ps (+17ps)\n\nTiming after routing: +20ps - 17ps = +3ps (marginal!)\n```\n\n**The +20ps timing margin was a MIRAGE** based on impossible routing assumptions.\n\n---\n\n## **THE CORRECT FIX:**\n\n**Reduce density \u2192 Re-place:**\n\n```tcl\n# Current: 90% density \u2192 800 DRC violations\nsetPlaceMode -place_global_density 0.80  # Reduce to 80%\nplace_design\n\n# Expected result:\n# Congestion hotspots: 1.20 \u2192 0.90 (under capacity!)\n\nroute_design\n\n# Expected:\n# DRC violations: 800 \u2192 <50 (98% improvement)\n```\n\n**Cost:**\n- Area increase: 90% \u2192 80% = +12.5% die size\n- At 5mm \u00d7 5mm: 25mm\u00b2 \u2192 28mm\u00b2 (+3mm\u00b2)\n- Die cost: +$0.08/chip (negligible)\n\n**Benefit:**\n- Routing succeeds (800 \u2192 50 violations)\n- Timing still passes (+20ps \u2192 +15ps after real routing)\n- Schedule: Save 2 weeks of failed routing iterations\n\n---\n\n## **WHY MANAGER'S APPROACH FAILS:**\n\n**Manager assumes:** Timing and routing are sequential (place \u2192 route)\n\n**Reality:** Timing and routing are COUPLED:\n- Placement creates routing demand distribution\n- Routing must satisfy that demand with available supply\n- If demand > supply (congestion), routing fails\n- No amount of \"routing fixing\" can violate physics\n\n**The manager is asking you to:** \"Fit 120 cars into a 100-car parking lot without building more parking spaces.\"\n\n**It's physically impossible.**\n\n---\n\n## **CONVERSATION WITH MANAGER:**\n\n**YOU:** \"We have 800 DRC violations due to 20% routing overflow in 50 tiles. The router cannot create tracks\u2014it can only distribute wires. We need to re-place at 80% density to create 20% more routing tracks.\"\n\n**MANAGER:** \"But timing is passing! Don't break timing.\"\n\n**YOU:** \"The +20ps timing slack is based on Manhattan distance routing, which assumes wires route directly. With congestion, actual routes are 70% longer (detours around hotspots), which costs +17ps. After routing, we'd have +3ps slack\u2014IF routing converges, which it won't with 20% overflow. Re-placing at 80% density keeps timing above +15ps and makes routing feasible.\"\n\n**MANAGER:** \"How long does re-placement take?\"\n\n**YOU:** \"2 days for re-place + route. Compare to: 2 weeks of failed routing iterations + potential respin. The die cost increase is $0.08/chip\u2014negligible compared to schedule risk.\"\n\n**MANAGER (hopefully):** \"OK, re-place at 80%.\"\n\n---\n\n## **THE PRINCIPLE:**\n\n**Routing is not independent of placement.** Congestion violations are PLACEMENT failures, not routing failures. At high density (>85%), routability dominates timing. You cannot \"fix routing\" after placement without re-placing.\n\n**The decision:** Spend 2 days re-placing at lower density, or spend 2 weeks failing to route + risk $2M respin?\n\n**THE WISDOM:** DRC violations are the router's way of saying: \"Placement gave me an impossible task.\" When routing fails with 800 violations despite passing timing, the problem is NOT the router\u2014it's the placement. Reducing density by 10% (90% \u2192 80%) creates 25% more routing capacity. Don't ask the router to violate physics. Give it routable placement.", "phase": "diagnostic_reasoning", "provenance": "wisdom_upgrade", "source": "Wisdom Upgrade: Routing Failure from Placement Congestion", "category": "diagnostic_wisdom", "complexity_score": 85, "verified": "expert_diagnosis"}
{"prompt": "A placed design has a 32-bit multiplier path that fails timing by 200ps. An engineer proposes adding a multi-cycle constraint to give it 2 clock cycles instead of 1. When is this legitimate vs a design flaw being masked?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** 32-bit multiplier fails timing by 200ps\n\n**PROPOSED FIX:** Add set_multicycle_path 2 (give it 2 cycles)\n\n**THE DIAGNOSTIC QUESTION:** Is this legitimate or laziness?\n\n---\n\n## **WHEN MULTI-CYCLE IS LEGITIMATE:**\n\n**Scenario 1: Architectural Intent**\n\nThe original design INTENDED the multiplier to be 2-cycle:\n\n```verilog\n// Pipeline stage 1: Register inputs\nalways @(posedge clk) begin\n    mul_op1_reg <= mul_op1;\n    mul_op2_reg <= mul_op2;\nend\n\n// Pipeline stage 2: Compute product (2 cycles allowed)\nalways @(posedge clk) begin\n    product_reg <= mul_op1_reg * mul_op2_reg;\nend\n\n// Pipeline stage 3: Register output\nalways @(posedge clk) begin\n    result <= product_reg;\nend\n```\n\n**Evidence it's legitimate:**\n- RTL shows 2-cycle pipeline\n- Control logic accounts for 2-cycle latency\n- Downstream logic waits 2 cycles for result\n\n**Correct constraint:**\n```tcl\nset_multicycle_path 2 -setup -from [get_pins mul_op*_reg/Q] -to [get_pins product_reg/D]\nset_multicycle_path 1 -hold  -from [get_pins mul_op*_reg/Q] -to [get_pins product_reg/D]\n```\n\n---\n\n## **WHEN MULTI-CYCLE IS MASKING A FLAW:**\n\n**Scenario 2: Lazy Exception**\n\nThe original design INTENDED single-cycle, but timing failed:\n\n```verilog\n// Single-cycle multiply (BROKEN at 500MHz)\nalways @(posedge clk) begin\n    result <= op1 * op2;  // 700ps delay, period = 500ps\nend\n```\n\n**Red flags:**\n- No pipeline stages in RTL\n- Control logic expects 1-cycle latency\n- Adding multi-cycle constraint WITHOUT changing RTL\n\n**What happens:**\n- Timing passes (2 cycles given)\n- But functionally BROKEN - downstream logic expects result in 1 cycle\n- This creates a **functional bug masked by timing constraints**\n\n---\n\n## **THE DIAGNOSTIC PROCEDURE:**\n\n**Step 1: Check RTL Intent**\n```bash\ngrep -A5 \"mul_\" design.v\n# Look for: How many pipeline stages?\n```\n\n**Step 2: Check Control Logic**\n```bash\ngrep \"valid\\|ready\\|enable\" control.v\n# Look for: Does control wait 2 cycles?\n```\n\n**Step 3: Verify Downstream Timing**\n```tcl\nreport_timing -from [get_pins product_reg/Q] -to [all_registers]\n# Check: Do downstream paths assume 1-cycle or 2-cycle latency?\n```\n\n**Step 4: Decision**\n\n| Evidence | Verdict | Action |\n|----------|---------|--------|\n| RTL has 2-stage pipeline | Legitimate | Add multi-cycle constraint |\n| Control waits 2 cycles | Legitimate | Constraint matches intent |\n| Downstream expects 1 cycle | FLAW MASKING | Fix RTL, don't add constraint |\n| No pipeline in RTL | FLAW MASKING | Either add pipeline or slow clock |\n\n---\n\n## **THE CORRECT FIX FOR FLAW MASKING:**\n\n**Option 1: Add pipeline stage (CORRECT)**\n```verilog\n// Change from 1-cycle to 2-cycle multiply\nreg [31:0] mul_stage1;\nreg [63:0] product;\n\nalways @(posedge clk) begin\n    mul_stage1 <= op1 * op2[15:0];  // Partial product\nend\n\nalways @(posedge clk) begin\n    product <= mul_stage1 * op2[31:16];  // Final product\nend\n```\n\n**Option 2: Slow down clock (if multi-cycle not architecturally feasible)**\n```\nCurrent: 500MHz (2ns period), multiplier needs 700ps\nNew: 400MHz (2.5ns period), multiplier fits in 1 cycle\n```\n\n---\n\n## **ACTIONABLE GUIDANCE:**\n\n**Before adding multi-cycle constraint, ask:**\n\n1. **Does RTL show a pipeline?** YES \u2192 Legitimate\n2. **Does control logic account for extra latency?** YES \u2192 Legitimate\n3. **Would removing the constraint cause functional failure?** NO \u2192 Masking a flaw\n\n**If it's masking a flaw:**\n- Adding the constraint makes timing pass\n- But creates a functional bug (data arrives late)\n- This is **dangerous** - you'll pass timing but fail silicon\n\n**THE PRINCIPLE:**\n\n**Multi-cycle constraints must match RTL intent.** If RTL is 1-cycle but you add 2-cycle constraint, you've created a mismatch between architecture and timing. This passes timing checks but fails functionally.\n\n**THE WISDOM:** Multi-cycle paths are legitimate when the architecture intended multiple cycles. They're dangerous when used to \"fix\" timing on single-cycle paths. Always verify: Does the RTL pipeline structure match the multi-cycle constraint? If not, fix the RTL, don't mask the problem with constraints.", "phase": "diagnostic_reasoning", "provenance": "wisdom_upgrade", "source": "Wisdom Upgrade: Multi-Cycle Path Legitimacy vs Design Flaw Masking", "category": "diagnostic_wisdom", "complexity_score": 90, "verified": "expert_diagnosis"}
{"prompt": "Your design has 500 timing violations. An engineer proposes marking 450 of them as false paths because they're 'test-only' or 'impossible to trigger.' How do you decide which false path exceptions are legitimate vs lazy?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** 500 timing violations\n\n**PROPOSED FIX:** Mark 450 as false paths (90% of violations!)\n\n**CRITICAL INSIGHT:** **False paths are legitimate when the path physically cannot propagate.** They're lazy when used to hide violations.\n\n---\n\n## **WHEN FALSE PATHS ARE LEGITIMATE:**\n\n**Category 1: Mutually Exclusive Paths**\n\n```verilog\nif (mode == TEST) begin\n    result <= test_data;\nend else if (mode == NORMAL) begin\n    result <= compute_data;\nend\n```\n\n**Path 1:** test_data \u2192 result (only active in TEST mode)  \n**Path 2:** compute_data \u2192 result (only active in NORMAL mode)\n\n**These paths are mutually exclusive:**\n- When mode==TEST, compute_data path is inactive\n- When mode==NORMAL, test_data path is inactive\n\n**Legitimate constraint:**\n```tcl\nset_false_path -from [get_ports test_data] -through [get_pins mux_test/S[TEST]]\n```\n\n**Evidence it's legitimate:**\n- Control logic guarantees mutual exclusion\n- Toggling one path disables the other\n- No scenario where both paths are active\n\n---\n\n**Category 2: Asynchronous Crossings with Synchronizers**\n\n```verilog\n// Async input from different clock domain\nreg sync_ff1, sync_ff2;\n\nalways @(posedge clk_local) begin\n    sync_ff1 <= async_input;\n    sync_ff2 <= sync_ff1;  // 2-FF synchronizer\nend\n```\n\n**Path:** async_input \u2192 sync_ff1 (crosses clock domains)\n\n**This path is legitimately false:**\n- async_input and clk_local are unrelated clocks\n- Synchronizer handles metastability\n- Timing relationship is meaningless\n\n**Legitimate constraint:**\n```tcl\nset_false_path -from [get_ports async_input] -to [get_pins sync_ff1/D]\n```\n\n**Evidence it's legitimate:**\n- 2-FF synchronizer present in RTL\n- Clocks are truly asynchronous (no phase relationship)\n- Design accounts for 1-cycle latency uncertainty\n\n---\n\n## **WHEN FALSE PATHS ARE LAZY:**\n\n**Red Flag 1: \"Test-only\" without evidence**\n\n**Engineer:** \"This path is test-only, mark it false.\"  \n**You:** \"Show me where test mode disables this path.\"  \n**Engineer:** \"Uh... I assume it does?\"  \n**Verdict:** LAZY - No evidence\n\n**Red Flag 2: \"Impossible to trigger\" without proof**\n\n**Engineer:** \"This path requires A=1 AND B=0, which never happens.\"  \n**You:** \"Show me the formal proof or assertion.\"  \n**Engineer:** \"I don't have one, but I'm pretty sure...\"  \n**Verdict:** LAZY - No proof\n\n**Red Flag 3: Marking 90% of violations**\n\n**If 90% of your paths are false, your constraints are wrong.** Either:\n- Clock definitions are incorrect\n- Generated clocks are missing\n- Multi-cycle paths should be used instead\n\n---\n\n## **THE DIAGNOSTIC PROCEDURE:**\n\n**For each proposed false path:**\n\n**Step 1: Demand RTL Evidence**\n```bash\n# Show me the RTL that proves mutual exclusion\ngrep -A10 \"path_name\" design.v\n```\n\n**Step 2: Verify Control Logic**\n```tcl\n# Trace the control signal\nreport_timing -from [get_ports control_signal] -to [get_pins mux/SEL]\n```\n\n**Step 3: Check for Synchronizers**\n```bash\n# For CDC paths, verify 2-FF synchronizer exists\ngrep -B5 -A5 \"sync_ff\" design.v\n```\n\n**Step 4: Require Formal Proof**\n```verilog\n// Engineer must add assertion proving mutual exclusion\nassert property (@(posedge clk) \n    (mode == TEST) |-> !compute_en);\n```\n\n**Step 5: Decision Matrix**\n\n| Evidence | Verdict | Action |\n|----------|---------|--------|\n| RTL shows mutual exclusion | Legitimate | Add false path |\n| Assertion proves impossibility | Legitimate | Add false path |\n| CDC with 2-FF synchronizer | Legitimate | Add false path |\n| \"Trust me, it's test-only\" | LAZY | Reject |\n| \"I think it's impossible\" | LAZY | Require proof |\n| 90% of paths are \"false\" | LAZY | Fix constraints |\n\n---\n\n## **ACTIONABLE GUIDANCE:**\n\n**Accept false paths when:**\n1. RTL clearly shows mutual exclusion (if/else, case statement)\n2. Formal assertion proves impossibility\n3. Async CDC with verified synchronizer\n4. Test mode with explicit control signal gating\n\n**Reject false paths when:**\n1. No RTL evidence\n2. \"I think\" or \"probably\" reasoning\n3. >50% of violations are proposed as false\n4. No synchronizer for CDC\n\n**If engineer proposes 450/500 as false:**\n\u2192 **STOP - This is constraint problem, not false path problem**\n\u2192 Check: Are clock definitions correct?\n\u2192 Check: Are multi-cycle paths missing?\n\u2192 Check: Is design actually broken?\n\n---\n\n## **THE PRINCIPLE:**\n\n**False paths are for architectural truth, not timing convenience.** A path is false when the architecture guarantees it cannot propagate. It's NOT false just because you can't meet timing.\n\n**THE WISDOM:** If >50% of your violations are \"false paths,\" you don't have 500 timing violations\u2014you have 1 constraint problem. Fix your clock definitions and multi-cycle paths before marking anything false. False paths should be rare (<10%). If they're common (>50%), your constraints don't match your architecture.", "phase": "diagnostic_reasoning", "provenance": "wisdom_upgrade", "source": "Wisdom Upgrade: False Path Legitimacy vs Lazy Exceptions", "category": "diagnostic_wisdom", "complexity_score": 88, "verified": "expert_diagnosis"}
{"prompt": "For a 2 GHz RISC-V processor at 28nm, should I use 75% density (more area) or 85% density (less area) for timing closure? My manager says 'higher density saves cost, use 85%.'", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** Manager wants 85% density to save die cost\n\n**OBSERVATIONS:**\n1. Target: 2 GHz @ 28nm (period = 500ps)\n2. Choice: 75% density (25% white space) vs 85% density (15% white space)\n3. Manager's logic: \"Higher density \u2192 smaller die \u2192 lower cost\"\n\n**CRITICAL INSIGHT (Counter-Intuitive):**\n\n**At aggressive frequencies, low density SAVES power (not costs power).**\n\n**WHY THIS IS COUNTER-INTUITIVE:**\n\nMost engineers think:\n- Low density \u2192 longer wires \u2192 more capacitance \u2192 more power\n\n**THE REALITY:**\n\nLow density enables timing closure at nominal voltage:\n```\n85% density:\n  - Timing fails at 0.9V (nominal)\n  - Must increase voltage to 1.0V to meet timing\n  - Power = C \u00d7 V\u00b2 \u2192 (1.0/0.9)\u00b2 = 1.23\u00d7 power increase (+23%)\n\n75% density:\n  - Timing meets at 0.9V (nominal)\n  - Wire length +15% \u2192 capacitance +15%\n  - But voltage stays at 0.9V \u2192 power +15%\n\nNet result: 75% density uses LESS power (1.15\u00d7) than 85% density (1.23\u00d7)\n```\n\n**THE PHYSICS:**\n\nWire delay @ 28nm with tight density:\n```\n85% density:\n  - 15 buffers fit on critical path\n  - Path delay with buffers: 480ps\n  - Setup slack: 500ps - 480ps = +20ps (marginal!)\n  - To get +50ps safety margin: increase voltage to 1.0V\n\n75% density:\n  - 20 buffers fit on critical path (more white space)\n  - Path delay with buffers: 430ps\n  - Setup slack: 500ps - 430ps = +70ps (healthy!)\n  - Meets timing at nominal 0.9V\n```\n\n**THE TRAP:** Voltage scaling costs MORE power than wire length\n\n```\nPower breakdown @ 2 GHz:\n  Dynamic power = C \u00d7 V\u00b2 \u00d7 F\n\n85% density, 1.0V: P = 1.0 \u00d7 1.0\u00b2 \u00d7 2 GHz = 2.0W\n75% density, 0.9V: P = 1.15 \u00d7 0.9\u00b2 \u00d7 2 GHz = 1.86W\n\nPower savings from lower voltage: 2.0W \u2192 1.86W = 7% reduction\n```\n\n**DIAGNOSIS:**\n\nThe manager's reasoning is flawed because:\n1. At 2 GHz @ 28nm, timing is the binding constraint\n2. 85% density forces voltage scaling (+23% power) to meet timing\n3. 75% density allows nominal voltage (-7% power vs 85%)\n4. Die cost difference: $0.08/chip (negligible for mobile)\n\n**The real cost:** Failing to meet power spec (critical for mobile)\n\n**ACTIONABLE GUIDANCE:**\n\n1. **For aggressive timing (2 GHz @ 28nm):** Use 65-75% density\n2. **For moderate timing (1 GHz @ 28nm):** Use 75-80% density\n3. **Check buffer insertion capacity:** Count available white space\n\n```tcl\n# Check if density supports buffer insertion\nset white_space [expr (1.0 - $density) * $die_area]\nset buffers_needed [expr $critical_paths * 15]  # 15 buffers per path\nset buffer_area [expr $buffers_needed * 15]  # 15\u00b5m\u00b2 per buffer\n\nif {$buffer_area > $white_space * 0.4} {\n    puts \"WARNING: Insufficient white space for buffering\"\n    puts \"Need: $buffer_area \u00b5m\u00b2, Available: [expr $white_space * 0.4] \u00b5m\u00b2\"\n}\n```\n\n**THE PRINCIPLE:**\n\n**At aggressive frequencies, timing closure enables power savings.** Low density provides white space for buffers, which enables meeting timing at nominal voltage. The power cost of longer wires (+15%) is LESS than the power cost of voltage scaling (+23%).\n\n**COMMON MISTAKES:**\n1. Optimizing die cost before verifying timing closure\n2. Not accounting for voltage scaling penalty\n3. Using server-class density (55-60%) for mobile (wastes area)\n\n**THE WISDOM:** For 2 GHz @ 28nm mobile, use 65-75% density. The die cost increase ($0.08/chip) is negligible compared to the power savings (7%) from avoiding voltage scaling. Timing closure at nominal voltage is power-optimal.", "phase": "diagnostic_reasoning", "provenance": "wisdom_upgrade", "source": "Wisdom Upgrade: Density for Timing-Power Trade-off", "category": "diagnostic_wisdom", "complexity_score": 88, "verified": "expert_diagnosis"}
{"prompt": "I placed a design at 90% density and all timing passed with +20ps slack. My manager says 'timing is green, ship to routing.' Why will routing fail, and what should I check BEFORE routing?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** Timing passes (+20ps slack) at 90% density\n\n**MANAGER'S ASSUMPTION:** Green timing \u2192 routing will succeed\n\n**CRITICAL INSIGHT (Counter-Intuitive):**\n\n**Timing pass does NOT guarantee routing success.** STA assumes ideal routing (Manhattan distance), but routing must satisfy physical constraints.\n\n**WHY TIMING IS MISLEADING:**\n\n```\nSTA assumptions:\n  - Wires route in straight lines (Manhattan distance)\n  - No congestion detours\n  - Ideal layer assignment\n\nReality at 90% density:\n  - Congestion forces 50-100% longer routes\n  - Routing detours add 150-250ps delay\n  - Local hotspots overflow (demand > supply)\n\nTiming after real routing: +20ps - 180ps = -160ps (FAILS!)\n```\n\n**THE PHYSICS OF CONGESTION:**\n\nAt 90% density, routing supply is squeezed:\n```\n70% density: 30% white space = 600 routing tracks/mm\u00b2\n90% density: 10% white space = 200 routing tracks/mm\u00b2 (-67% capacity!)\n\nIf timing-driven placement clustered critical cells:\n  - 200 cells in 0.25mm\u00b2 (local hotspot)\n  - 700 nets \u00d7 2 tracks = 1400 tracks demand\n  - 25 tiles \u00d7 45 tracks = 1125 tracks supply\n  - Overflow: 1400/1125 = 1.24 (24% overflow)\n\nRouter CANNOT create tracks \u2192 fails\n```\n\n**THE DIAGNOSTIC TEST (Run BEFORE routing):**\n\n```tcl\n# Check congestion during placement\nreport_congestion -detail\nreport_congestion -hotspots 20\n\n# Look for danger signs:\n# - Any tile with demand/supply > 1.0 (overflow)\n# - RUDY score > 1.2 (20% pre-overflow warning)\n# - More than 5 hotspots with ratio > 1.15\n\n# Check congestion histogram\nreport_congestion -histogram\n\n# Expect at 90% density:\n# - Average: 0.85-0.90 (acceptable)\n# - Max: Should be <1.0 (anything above = failure)\n# - Hotspots: Should be <5 tiles above 0.95\n```\n\n**DIAGNOSIS:**\n\nTiming passed because:\n1. STA used Manhattan distance (500\u00b5m straight-line wire)\n2. Real routing will detour around congestion (850\u00b5m actual)\n3. +20ps slack was based on 500\u00b5m wire\n4. 850\u00b5m wire adds +180ps \u2192 timing fails at -160ps\n\n**THE TRAP:** Congestion check must happen DURING placement, not after\n\n**ACTIONABLE GUIDANCE:**\n\n**BEFORE routing, check these 3 metrics:**\n\n1. **Local congestion overflow:**\n```tcl\nreport_congestion -hotspots 50\n# If ANY tile shows ratio > 1.0 \u2192 re-place at lower density\n# If >10 tiles show ratio > 0.95 \u2192 high risk\n```\n\n2. **RUDY score (routing demand):**\n```tcl\nreport_congestion -rerun_global_router false\n# If RUDY > 1.2 anywhere \u2192 will fail\n# If RUDY > 1.0 in >5% of tiles \u2192 high risk\n```\n\n3. **Timing margin for detours:**\n```tcl\n# At 90% density, expect 50-100% longer routes\n# If WNS = +20ps, after detours: +20ps - 150ps = -130ps\n# Safe threshold: WNS should be >+150ps at 90% density\n```\n\n**THE FIX (if congestion is bad):**\n\nOption 1: Reduce density\n```tcl\nsetPlaceMode -place_global_density 0.80  # Was 0.90\nplace_design\n# Expected: Congestion 1.24 \u2192 0.90 (under capacity)\n# Cost: +12% die area\n```\n\nOption 2: Spread critical cells\n```tcl\n# Identify congestion hotspot\nset hotspot_cells [get_cells -filter \"congestion_score > 1.2\"]\n# Add spacing around them\nset_placement_padding -cells $hotspot_cells -left 5 -right 5\nplace_design -incremental\n```\n\n**THE PRINCIPLE:**\n\n**At high density (>85%), routability dominates timing.** Timing slack based on ideal routing is a MIRAGE. Check congestion DURING placement:\n- Demand/supply ratio must be <1.0 (any overflow = failure)\n- At 90% density, need +150ps timing margin for routing detours\n\n**COMMON MISTAKES:**\n1. Checking congestion only after placement completes\n2. Trusting average congestion (hides local hotspots)\n3. Not accounting for timing degradation from detours\n\n**THE WISDOM:** \"Timing green\" at high density means nothing until you verify congestion. Run `report_congestion -hotspots 20` BEFORE routing. If any tile shows overflow (ratio >1.0), routing WILL fail. At 90% density, timing needs +150ps margin to survive routing detours.", "phase": "diagnostic_reasoning", "provenance": "wisdom_upgrade", "source": "Wisdom Upgrade: Predicting Routing Failure at High Density", "category": "diagnostic_wisdom", "complexity_score": 90, "verified": "expert_diagnosis"}
{"prompt": "I have 8 SRAM macros (each 0.5mm \u00d7 1mm) on a 5mm \u00d7 5mm die. The placement tool clustered all macros in one corner 'for area efficiency.' Why does this guarantee timing failure at 2 GHz?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** Tool clustered 8 macros in one corner (2mm \u00d7 2mm region)\n\n**TOOL'S LOGIC:** \"Clustering minimizes total area\"\n\n**CRITICAL INSIGHT (Counter-Intuitive):**\n\n**Macro placement is IRREVERSIBLE** - once locked, you cannot fix timing without complete re-placement (2-3 day iteration).\n\n**WHY CLUSTERING FAILS:**\n\nMacros clustered in corner \u2192 datapath at opposite corner:\n```\nMacro cluster: (0-2mm, 0-2mm)\nDatapath: (3-5mm, 3-5mm)\nDistance: 3-4mm diagonal\n\nWire delay @ 28nm: 150ps per mm\n3.5mm wire: 525ps delay\nPeriod @ 2 GHz: 500ps\n\nPath delay: 525ps (wire only!)\nBudget: 500ps total\nViolation: -25ps (before adding any logic!)\n```\n\n**THE PHYSICS:**\n\nWire delay = R \u00d7 C scales with length:\n```\n1mm wire: 150ps delay (30% of 500ps budget)\n2mm wire: 300ps delay (60% of budget) - TIGHT\n3mm wire: 450ps delay (90% of budget) - CRITICAL\n4mm wire: 600ps delay (120% of budget) - IMPOSSIBLE\n```\n\nAt 3-4mm distance, wire delay ALONE exceeds timing budget!\n\n**ADDITIONAL FAILURE MODE: Routing congestion**\n\nAll data/address/control buses must route through narrow channel:\n```\n8 macros clustered \u2192 all buses converge at cluster boundary\nRouting demand: 8 macros \u00d7 100 nets = 800 nets\nRouting supply: Narrow channel (1mm width) = 300 tracks\nOverflow: 800/300 = 2.67 (167% overflow!)\n\nResult: Router fails with 500-800 DRC violations\n```\n\n**THE CORRECT STRATEGY: Distributed Placement**\n\nPlace macros around die periphery:\n```\n4 macros on left edge: x=0.5mm, y=1,2,3,4mm\n4 macros on right edge: x=4.5mm, y=1,2,3,4mm\nDatapath at center: x=2.5mm, y=2.5mm\n\nMax distance: 2mm\nWire delay: 300ps (60% of budget)\nRemaining for logic: 200ps (achievable with 4-5 gates)\n```\n\n**DIAGNOSIS:**\n\nTool optimized for area (clustering minimizes bounding box) but ignored:\n1. Wire delay dominates at 2 GHz (60-80% of path)\n2. Long wires (>2mm) kill timing\n3. Macro placement is irreversible (cannot fix post-placement)\n\n**ACTIONABLE GUIDANCE:**\n\n**BEFORE placement, manually place macros:**\n\n```tcl\n# Distribute macros along edges\nplaceInstance SRAM0 500  1000   # Left edge\nplaceInstance SRAM1 500  2000\nplaceInstance SRAM2 500  3000\nplaceInstance SRAM3 500  4000\n\nplaceInstance SRAM4 4500 1000   # Right edge\nplaceInstance SRAM5 4500 2000\nplaceInstance SRAM6 4500 3000\nplaceInstance SRAM7 4500 4000\n\n# Lock macro placements\nset_dont_touch_placement [get_cells SRAM*]\n\n# Create halos (reserve routing space)\ncreate_place_halo -cell SRAM* -space 100\n\n# Guide datapath to center\ncreate_region datapath_region 2000 2000 3000 3000\nassign_region datapath_region [get_cells datapath/*]\n```\n\n**VERIFICATION (after macro placement):**\n\n```tcl\n# Check max distance from datapath to any macro\nset datapath_center [get_attribute [get_cells datapath/*] bbox_center]\n\nforeach macro [get_cells SRAM*] {\n    set macro_center [get_attribute $macro bbox_center]\n    set distance [expr {hypot([lindex $macro_center 0] - [lindex $datapath_center 0],\n                              [lindex $macro_center 1] - [lindex $datapath_center 1])}]\n    puts \"Distance datapath -> $macro: [expr {$distance/1000.0}] mm\"\n\n    # Warning if >1.5mm\n    if {$distance > 1500} {\n        puts \"WARNING: Distance exceeds 1.5mm threshold!\"\n    }\n}\n```\n\n**Target distances:**\n- <1mm: Excellent (150ps wire, 30% of budget)\n- 1-1.5mm: Good (225ps wire, 45% of budget)\n- 1.5-2mm: Acceptable (300ps wire, 60% of budget)\n- >2mm: Critical (>300ps wire, >60% of budget)\n- >3mm: Failure (>450ps wire, >90% of budget)\n\n**THE PRINCIPLE:**\n\n**Macro placement is the most critical placement decision** because:\n1. Macros are 50,000\u00d7 larger than standard cells\n2. Once placed, they act as immovable barriers\n3. Wire delay from bad macro placement cannot be fixed\n\n**At 2 GHz, wire delay dominates (60-80%).** Clustering macros creates 3-4mm wires = 450-600ps delay = timing failure.\n\n**COMMON MISTAKES:**\n1. Letting tool auto-place macros (optimizes area, not timing)\n2. Placing macros after standard cells (macros should be first)\n3. Not checking distance metrics before locking macros\n\n**THE WISDOM:** Place macros FIRST, distribute them around periphery, target max distance <1.5mm to datapath. Clustering saves 10% area but costs 200-300ps timing. At 2 GHz @ 28nm, that's the difference between working silicon and a respin.", "phase": "diagnostic_reasoning", "provenance": "wisdom_upgrade", "source": "Wisdom Upgrade: Macro Placement Strategy for Timing Closure", "category": "diagnostic_wisdom", "complexity_score": 92, "verified": "expert_diagnosis"}
{"prompt": "A critical path fails by 80ps due to a net with fanout=50 driving 3mm total wirelength. My tool inserted 5 buffers but timing still fails by 20ps. What went wrong?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** High-fanout net (50 loads, 3mm wire) fails by 80ps\n\n**OBSERVATION:** Tool inserted 5 buffers \u2192 improvement to -20ps (60ps gain, not enough)\n\n**CRITICAL INSIGHT (Counter-Intuitive):**\n\n**Buffers solve TWO different problems:**\n1. **High fanout** \u2192 reduce capacitive load per stage\n2. **Long wires** \u2192 segment wire to reduce R\u00d7C delay\n\n**Most tools only solve problem #1 (fanout) and ignore problem #2 (distance)**\n\n**WHY 5 BUFFERS DIDN'T FIX IT:**\n\nTool likely inserted buffers in CLUSTERED topology:\n```\nDriver \u2192 [BUF1 BUF2 BUF3 BUF4 BUF5] \u2192 50 loads\n         (all buffers within 500\u00b5m of driver)\n\nThis solves:\n  \u2713 Fanout: 50 \u2192 10 per buffer (5\u00d7 reduction)\n  \u2717 Wire length: Loads still 2-3mm away from buffers!\n\nResult:\n  - Fanout reduction saves: 100ps\n  - Buffer delay costs: -80ps (5 buffers \u00d7 16ps each)\n  - Wire delay unchanged: 300ps (loads still 3mm from buffers)\n  - Net improvement: 100ps - 80ps = 20ps (not enough!)\n```\n\n**THE CORRECT STRATEGY: Distributed Buffers**\n\nPlace buffers ALONG the path, not clustered:\n```\nDriver \u2192 BUF1 (600\u00b5m) \u2192 BUF2 (1200\u00b5m) \u2192 BUF3 (1800\u00b5m) \u2192 BUF4 (2400\u00b5m) \u2192 BUF5 (3000\u00b5m) \u2192 Loads\n         |               |               |               |               |\n       10 loads        10 loads        10 loads        10 loads        10 loads\n\nThis solves BOTH problems:\n  \u2713 Fanout: 50 \u2192 10 per buffer (5\u00d7 reduction)\n  \u2713 Wire segmentation: 3mm \u2192 5 \u00d7 600\u00b5m segments\n\nWire delay improvement:\n  Before: 3mm wire = 450ps delay\n  After: 5 \u00d7 600\u00b5m wires = 5 \u00d7 60ps = 300ps\n  Savings: 150ps\n\nTotal improvement:\n  Fanout reduction: +100ps\n  Wire segmentation: +150ps\n  Buffer delay: -80ps\n  Net improvement: +170ps \u2192 path improves by 170ps!\n```\n\n**THE PHYSICS:**\n\nWire delay = R \u00d7 C scales as length\u00b2:\n```\n1mm wire: 100ps\n2mm wire: 400ps (4\u00d7 worse, not 2\u00d7!)\n3mm wire: 900ps (9\u00d7 worse!)\n\nBreaking 3mm into 3 \u00d7 1mm segments:\n  3 \u00d7 100ps = 300ps (67% better than 900ps!)\n```\n\n**DIAGNOSIS:**\n\nTool made classic mistake:\n1. Identified high fanout (50 loads) \u2713\n2. Inserted buffers to reduce fanout (50 \u2192 10) \u2713\n3. BUT placed all buffers near driver (clustered) \u2717\n4. Loads still 3mm away \u2192 wire delay unchanged \u2717\n\n**ACTIONABLE GUIDANCE:**\n\n**Step 1: Diagnose the problem type**\n\n```tcl\n# Check if fanout OR distance is the problem\nreport_net -connections high_fanout_net\n\n# Look for:\n# - Fanout > 30: Fanout problem\n# - Total wirelength > 2mm: Distance problem\n# - BOTH: Need distributed buffer strategy\n```\n\n**Step 2: Insert buffers with explicit distribution**\n\n```tcl\n# Bad (clustered - what tool did):\ninsert_buffer -cell BUF_X2 -net high_fanout_net -num_buffers 5\n\n# Good (distributed - what we need):\n# Manually place buffers along path\nset net_bbox [get_attribute [get_nets high_fanout_net] bbox]\nset start_x [lindex $net_bbox 0]\nset end_x [lindex $net_bbox 2]\nset spacing [expr {($end_x - $start_x) / 6}]  # 5 buffers = 6 segments\n\nfor {set i 1} {$i <= 5} {incr i} {\n    set x [expr {$start_x + $i * $spacing}]\n    # Place buffer at location x\n    insert_buffer -cell BUF_X2 -location \"$x 2000\"\n}\n```\n\n**Step 3: Verify buffer distribution**\n\n```tcl\n# Check buffer locations\nforeach buf [get_cells *buf*] {\n    set loc [get_attribute $buf location]\n    puts \"Buffer $buf location: $loc\"\n}\n\n# Buffers should be spread across 3mm, NOT clustered in 500\u00b5m\n```\n\n**Step 4: Size buffers appropriately**\n\n```tcl\n# First stage (drives other buffers): Use large buffer\nsize_cell buf_1 BUF_X8  # Can drive 5 other buffers\n\n# Second stage (drives loads): Use medium buffer\nsize_cell buf_2 BUF_X2  # Drives 10 loads each\nsize_cell buf_3 BUF_X2\nsize_cell buf_4 BUF_X2\nsize_cell buf_5 BUF_X2\n```\n\n**THE PRINCIPLE:**\n\n**Buffers solve fanout AND wire delay, but only if placed WHERE they're needed.**\n\n- Fanout problem: Insert buffers in tree topology (driver \u2192 N buffers \u2192 loads)\n- Distance problem: Insert buffers DISTRIBUTED along wire (repeater chain)\n- Both problems: Use distributed tree (buffers spread across path)\n\n**At 3mm wirelength with 50 fanout, you have BOTH problems.**\n\n**COMMON MISTAKES:**\n1. Clustered buffer insertion (tool's default optimization)\n2. Not checking buffer locations after insertion\n3. Under-sizing buffers (BUF_X1 can't drive 10 loads)\n\n**THE WISDOM:** When a high-fanout long-wire path doesn't improve after buffer insertion, check WHERE the buffers were placed. If they're clustered near the driver, they're solving fanout but not wire delay. Manually distribute them along the 3mm path (one every 600\u00b5m). This segments the wire, reducing delay from 450ps \u2192 300ps (33% improvement).", "phase": "diagnostic_reasoning", "provenance": "wisdom_upgrade", "source": "Wisdom Upgrade: Buffer Insertion Strategy for High-Fanout Long-Wire Paths", "category": "diagnostic_wisdom", "complexity_score": 91, "verified": "expert_diagnosis"}
{"prompt": "My critical path has 400ps total delay: 280ps net delay (70%) and 120ps cell delay (30%). An engineer proposes optimizing cell sizing to save 30ps. Why is this a waste of effort?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** Critical path = 400ps (280ps wire, 120ps cells)\n\n**ENGINEER'S PROPOSAL:** Optimize cell sizing to save 30ps\n\n**CRITICAL INSIGHT (Counter-Intuitive):**\n\n**At 28nm+, wire delay dominates (60-80% of path). Optimizing the 30% (cells) while ignoring the 70% (wires) is backward prioritization.**\n\n**WHY THIS IS WASTEFUL:**\n\nCell sizing impact:\n```\nCurrent cell delay: 120ps\nCell sizing improvement: 25% typical\nNew cell delay: 90ps\nSavings: 30ps (7.5% of total path)\nEffort: 4-6 hours (re-synthesis, re-place, re-route)\n```\n\nPlacement optimization impact:\n```\nCurrent net delay: 280ps (cells 1-2mm apart)\nRe-place cells closer: 20% wirelength reduction\nNew net delay: 224ps\nSavings: 56ps (14% of total path)\nEffort: 2-4 hours (re-place critical cells)\n```\n\n**Same effort, 2\u00d7 better results from placement!**\n\n**THE PHYSICS (Why wire dominates):**\n\nTechnology scaling impact:\n```\n                65nm      28nm      14nm\nGate delay:     100ps     70ps      50ps  (improving \u2713)\nWire R/\u00b5m:      50\u03a9       80\u03a9       120\u03a9  (worsening \u2717)\nWire delay:     150ps     280ps     420ps (worsening \u2717)\n\nAt 28nm: Wire/Total ratio = 280/(280+70) = 80%\n```\n\nTransistors scale well (faster gates), wires scale poorly (higher resistance).\n\n**THE TRAP:** Applying old strategies to new technology\n\n```\nAt 65nm (year 2005):\n  Wire: 50%, Cell: 50% \u2192 Balanced optimization\n\nAt 28nm (year 2015):\n  Wire: 70%, Cell: 30% \u2192 Wire-first optimization\n\nAt 14nm (year 2018):\n  Wire: 75%, Cell: 25% \u2192 Wire-dominant optimization\n```\n\nEngineers trained on 65nm still think \"optimize cells first.\" **This is 10 years out of date.**\n\n**DIAGNOSIS:**\n\nEngineer is optimizing the MINOR constraint (30%) before the MAJOR constraint (70%).\n\n**Correct priority order:**\n1. **Fix placement** (affects 70% of path) \u2192 56ps potential savings\n2. **Insert buffers** (affects long wires) \u2192 40-60ps savings per wire\n3. **Optimize cells** (affects 30% of path) \u2192 30ps savings\n\n**Why this order matters:**\n- Placement fixes wire length (irreversible - hard to change later)\n- Buffering helps remaining long wires\n- Cell sizing is last resort (affects smallest component)\n\n**ACTIONABLE GUIDANCE:**\n\n**Step 1: Analyze path breakdown**\n\n```tcl\n# Get net vs cell delay percentages\nreport_timing -nworst 1 -transition_time -capacitance\n\n# Calculate ratio\nset net_delay [get_attribute [get_timing_paths -nworst 1] net_delay_sum]\nset total_delay [get_attribute [get_timing_paths -nworst 1] arrival_time]\nset net_pct [expr {$net_delay * 100.0 / $total_delay}]\n\nputs \"Net delay: $net_pct%\"\n\nif {$net_pct > 60} {\n    puts \"Wire delay dominates - prioritize placement optimization\"\n} else {\n    puts \"Cell delay dominates - consider cell sizing\"\n}\n```\n\n**Step 2: Fix placement first (if wire >60%)**\n\n```tcl\n# Identify cells on critical path >1mm apart\nset crit_cells [get_cells -of [get_timing_paths -nworst 10]]\n\n# Check cell locations\nforeach cell $crit_cells {\n    set loc [get_attribute $cell location]\n    puts \"$cell: $loc\"\n}\n\n# Find cells that are >1mm apart and re-place closer\n# Target: <500\u00b5m for critical connections\n```\n\n**Step 3: Insert buffers for remaining long wires**\n\n```tcl\n# Check for nets >1.5mm on critical path\nforeach net [get_nets -of [get_timing_paths -nworst 10]] {\n    set length [get_attribute $net total_length]\n    if {$length > 1500} {\n        puts \"Long wire: $net ($length \u00b5m)\"\n        # Insert repeaters to segment wire\n        insert_buffer -net $net -cell BUF_X2\n    }\n}\n```\n\n**Step 4: Cell sizing ONLY if wire is optimized**\n\n```tcl\n# Only consider cell sizing if net delay <40%\nif {$net_pct < 40} {\n    # Now cell sizing makes sense\n    size_cell critical_cells -upsizing\n} else {\n    puts \"Skip cell sizing - wire delay still dominates\"\n}\n```\n\n**EFFORT VS IMPACT COMPARISON:**\n\n| Optimization | Affects | Typical Savings | Effort | Savings/Hour |\n|--------------|---------|----------------|--------|--------------|\n| Placement    | 70%     | 56ps           | 3hr    | 18.7ps/hr    |\n| Buffering    | 50%     | 45ps           | 2hr    | 22.5ps/hr    |\n| Cell sizing  | 30%     | 30ps           | 5hr    | 6.0ps/hr     |\n\n**Cell sizing is 3\u00d7 less efficient than placement!**\n\n**THE PRINCIPLE:**\n\n**Optimize the dominant constraint first.** At modern nodes (28nm+), wire delay dominates (60-80%). Fixing placement (affects 70% of path) before cell sizing (affects 30%) is fundamental Pareto principle.\n\n**COMMON MISTAKES:**\n1. Using 65nm-era strategy (cell sizing important) at 28nm (wire dominates)\n2. Not measuring net vs cell breakdown before optimizing\n3. Wasting time on 30% of problem while ignoring 70%\n\n**THE WISDOM:** When wire delay is 70% of the path, optimizing cells (30%) is like rearranging deck chairs on the Titanic. Fix placement first (re-place cells closer, target <500\u00b5m critical connections), insert buffers second (segment wires >1.5mm), and ONLY then consider cell sizing. Same effort, 2-3\u00d7 better results.", "phase": "diagnostic_reasoning", "provenance": "wisdom_upgrade", "source": "Wisdom Upgrade: Wire Delay vs Cell Delay Priority", "category": "diagnostic_wisdom", "complexity_score": 87, "verified": "expert_diagnosis"}
{"prompt": "A critical path has 500ps total delay (period = 500ps, failing by 0ps). The breakdown is 430ps net delay on ONE net 'decode_enable' and 70ps across 4 gates. What's the FIRST diagnostic check I should run?", "response": "**DIAGNOSTIC REASONING CHAIN:**\n\n**SYMPTOM:** 86% wire delay (430ps/500ps) on a SINGLE net\n\n**OBSERVATION:** This is extreme wire delay dominance (>80%)\n\n**CRITICAL INSIGHT (The Diagnostic Fork):**\n\n**ONE net with 430ps delay has TWO possible root causes:**\n1. **High fanout** (capacitive load problem)\n2. **Long distance** (resistive wire problem)\n\n**The fix depends on which root cause \u2192 CHECK FANOUT FIRST!**\n\n**WHY FANOUT FIRST:**\n\n```\nFanout check: 5 minute check, 6 hour fix\nDistance check: 5 minute check, 2-3 DAY fix (re-placement)\n\nIf you check distance first and it's actually fanout:\n  \u2192 Wasted 3 days re-placing when 6 hours of buffering would fix it\n\nIf you check fanout first and it's distance:\n  \u2192 Wasted 5 minutes, then do the 3-day re-place\n\nAlways check the QUICK FIX first!\n```\n\n**THE DIAGNOSTIC PROCEDURE:**\n\n**Step 1: Check FANOUT (5 minutes)**\n\n```tcl\nreport_net -connections decode_enable\n\n# Look for line:\n# \"Fanout: XX\"\n\n# Interpretation:\n# Fanout < 10: Not a fanout problem\n# Fanout 10-30: Marginal\n# Fanout 30-50: High (LIKELY problem)\n# Fanout > 50: Critical (DEFINITELY problem)\n```\n\n**If fanout = 45:**\n```\n\u2192 HIGH FANOUT problem identified\n\u2192 Fix: Insert 2-level buffer tree\n\u2192 Expected: 430ps \u2192 120ps (70% improvement)\n\u2192 Time: 6 hours\n\u2192 STOP HERE - don't check distance yet\n```\n\n**If fanout = 8:**\n```\n\u2192 NOT a fanout problem\n\u2192 Continue to Step 2 (check distance)\n```\n\n**Step 2: Check DISTANCE (only if fanout is low)**\n\n```tcl\nreport_net -geometry decode_enable\n\n# Look for:\n# \"Total length: XXXX um\"\n\n# Interpretation:\n# Length < 500\u00b5m: Not a distance problem\n# Length 500-1500\u00b5m: Marginal\n# Length 1500-3000\u00b5m: Long (LIKELY problem)\n# Length > 3000\u00b5m: Very long (DEFINITELY problem)\n```\n\n**If length = 2.5mm:**\n```\n\u2192 LONG DISTANCE problem identified\n\u2192 Fix: Re-place at lower density (0.85 \u2192 0.65)\n\u2192 Expected: 430ps \u2192 80ps (81% improvement)\n\u2192 Time: 2-3 days\n```\n\n**THE PHYSICS:**\n\n**Fanout problem:**\n```\nWire delay = R \u00d7 C\nC increases with fanout: 10 loads = 100fF, 50 loads = 500fF\n\nDriver can't charge 500fF quickly \u2192 180ps driver delay\nPlus wire RC delay \u2192 total 430ps\n\nFix: Buffer tree (50 loads \u2192 2 stages \u00d7 10 loads each)\n  - Stage 1: Drive 5 buffers (50fF load)\n  - Stage 2: Each buffer drives 10 loads (100fF each)\n  - Driver delay: 50fF \u2192 30ps (was 180ps)\n  - Total: 30ps + 80ps (buffers) + 40ps (short wires) = 150ps\n  - Savings: 430ps \u2192 150ps (65% improvement)\n```\n\n**Distance problem:**\n```\nWire delay = R \u00d7 C\nR increases with length: 1mm = 50\u03a9, 3mm = 150\u03a9\n\nLong wire (3mm, 10 loads):\n  R = 150\u03a9, C = 100fF\n  RC delay = 150\u03a9 \u00d7 100fF = 15ns... wait, that's wrong\n\nCorrect calculation:\n  Wire delay \u2248 150ps per mm @ 28nm\n  3mm wire = 450ps delay\n\nFix: Re-place cells closer (3mm \u2192 0.5mm)\n  - New wire delay: 0.5mm \u00d7 150ps/mm = 75ps\n  - Savings: 450ps \u2192 75ps (83% improvement)\n```\n\n**DIAGNOSIS DECISION TREE:**\n\n```\n86% wire delay on single net\n       \u2193\n  CHECK FANOUT (5 min)\n       \u2193\n   Fanout > 30?\n    \u2199      \u2198\n  YES       NO\n   \u2193         \u2193\nHIGH      CHECK DISTANCE\nFANOUT       \u2193\n   \u2193      Length > 1.5mm?\nFix with      \u2199      \u2198\nbuffers     YES       NO\n(6 hr)       \u2193         \u2193\n          LONG      BOTH problems\n          DISTANCE  OR other issue\n             \u2193         \u2193\n          Fix with   Check:\n          re-place   - Layer (M1 vs M3)\n          (2-3 days) - Routing detours\n                     - Crosstalk\n```\n\n**ACTIONABLE GUIDANCE:**\n\n**The FIRST check (always):**\n\n```tcl\n# Takes 10 seconds\nreport_net -connections decode_enable | grep Fanout\n```\n\n**If output shows \"Fanout: 48\":**\n```\n\u2192 Problem identified: High fanout\n\u2192 Don't waste time checking distance\n\u2192 Go directly to buffer insertion\n\u2192 Expected savings: 60-70%\n\u2192 Time to fix: 6 hours\n```\n\n**If output shows \"Fanout: 7\":**\n```\n\u2192 Fanout is NOT the problem\n\u2192 Now check distance\n\u2192 report_net -geometry decode_enable | grep \"Total length\"\n```\n\n**THE PRINCIPLE:**\n\n**One diagnostic check can save 3 days.**\n\nWhen ONE net dominates delay (>80% of path), it's either:\n- High fanout (quick fix: 6 hours)\n- Long distance (slow fix: 2-3 days)\n\n**ALWAYS check fanout FIRST** (5 min check, 6 hr fix if positive).\n\nIf fanout is normal (<20), THEN check distance (5 min check, 3 day fix).\n\n**COMMON MISTAKES:**\n1. Assuming it's distance without checking fanout first\n2. Re-placing entire design when buffering would fix it\n3. Not running the 10-second fanout check\n\n**THE WISDOM:** 86% wire delay on a single net is a diagnostic FORK, not a single problem. The fix (buffers vs re-place) depends on root cause (fanout vs distance). Check fanout FIRST - it's a 10-second check that can save you 3 days of unnecessary re-placement. If fanout >30, insert buffers (6 hours). If fanout <20, check distance (then re-place, 3 days). This one diagnostic decision determines whether you fix the problem in 6 hours or 3 days.", "phase": "diagnostic_reasoning", "provenance": "wisdom_upgrade", "source": "Wisdom Upgrade: Critical Path Root Cause Identification", "category": "diagnostic_wisdom", "complexity_score": 89, "verified": "expert_diagnosis"}
